Published as a conference paper at ICLR 2015
 ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION
 Diederik P. Kingma*
 University of Amsterdam, OpenAI
 dpkingma@openai.com
 ABSTRACT
 arXiv:1412.6980v9  [cs.LG]  30 Jan 2017
 Jimmy Lei Ba∗
 University of Toronto
 jimmy@psi.utoronto.ca
 We introduce Adam, an algorithm for first-order gradient-based optimization of
 stochastic objective functions, based on adaptive estimates of lower-order mo
ments. The method is straightforward to implement, is computationally efficient,
 has little memory requirements, is invariant to diagonal rescaling of the gradients,
 and is well suited for problems that are large in terms of data and/or parameters.
 The method is also appropriate for non-stationary objectives and problems with
 very noisy and/or sparse gradients. The hyper-parameters have intuitive interpre
tations and typically require little tuning. Some connections to related algorithms,
 on which Adam was inspired, are discussed. We also analyze the theoretical con
vergence properties of the algorithm and provide a regret bound on the conver
gence rate that is comparable to the best known results under the online convex
 optimization framework. Empirical results demonstrate that Adam works well in
 practice and compares favorably to other stochastic optimization methods. Finally,
 we discuss AdaMax, a variant of Adam based on the infinity norm.
 1 INTRODUCTION
 Stochastic gradient-based optimization is of core practical importance in many fields of science and
 engineering. Many problems in these fields can be cast as the optimization of some scalar parameter
ized objective function requiring maximization or minimization with respect to its parameters. If the
 function is differentiable w.r.t. its parameters, gradient descent is a relatively efficient optimization
 method, since the computation of first-order partial derivatives w.r.t. all the parameters is of the same
 computational complexity as just evaluating the function. Often, objective functions are stochastic.
 For example, many objective functions are composed of a sum of subfunctions evaluated at different
 subsamples of data; in this case optimization can be made more efficient by taking gradient steps
 w.r.t. individual subfunctions, i.e. stochastic gradient descent (SGD) or ascent. SGD proved itself
 as an efficient and effective optimization method that was central in many machine learning success
 stories, such as recent advances in deep learning (Deng et al., 2013; Krizhevsky et al., 2012; Hinton
 &Salakhutdinov, 2006; Hinton et al., 2012a; Graves et al., 2013). Objectives may also have other
 sources of noise than data subsampling, such as dropout (Hinton et al., 2012b) regularization. For
 all such noisy objectives, efficient stochastic optimization techniques are required. The focus of this
 paper is on the optimization of stochastic objectives with high-dimensional parameters spaces. In
 these cases, higher-order optimization methods are ill-suited, and discussion in this paper will be
 restricted to first-order methods.
 We propose Adam, a method for efficient stochastic optimization that only requires first-order gra
dients with little memory requirement. The method computes individual adaptive learning rates for
 different parameters from estimates of first and second moments of the gradients; the name Adam
 is derived from adaptive moment estimation. Our method is designed to combine the advantages
 of two recently popular methods: AdaGrad (Duchi et al., 2011), which works well with sparse gra
dients, and RMSProp (Tieleman & Hinton, 2012), which works well in on-line and non-stationary
 settings; important connections to these and other stochastic optimization methods are clarified in
 section 5. Some of Adam’s advantages are that the magnitudes of parameter updates are invariant to
 rescaling of the gradient, its stepsizes are approximately bounded by the stepsize hyperparameter,
 it does not require a stationary objective, it works with sparse gradients, and it naturally performs a
 form of step size annealing.
 ∗Equal contribution. Author ordering determined by coin flip over a Google Hangout.
 1
Published as a conference paper at ICLR 2015
 Algorithm 1: Adam, our proposed algorithm for stochastic optimization. See section 2 for details,
 and for a slightly more efficient (but less clear) order of computation. g2
 t indicates the elementwise
 square gt ⊙ gt. Good default settings for the tested machine learning problems are α = 0.001,
 β1 = 0.9, β2 = 0.999 and ϵ = 10−8. All operations on vectors are element-wise. With βt
 1 and βt
 2
 we denote β1 and β2 to the power t.
 Require: α: Stepsize
 Require: β1,β2 ∈ [0,1): Exponential decay rates for the moment estimates
 Require: f(θ): Stochastic objective function with parameters θ
 Require: θ0: Initial parameter vector
 m0 ←0(Initialize 1st moment vector)
 v0 ←0(Initialize 2nd moment vector)
 t ←0(Initialize timestep)
 while θt not converged do
 t ←t+1
 gt ←∇θft(θt−1) (Get gradients w.r.t. stochastic objective at timestep t)
 mt ←β1·mt−1 +(1−β1)·gt (Update biased first moment estimate)
 b
 b
 vt ←β2 ·vt−1 +(1−β2)·g2
 t (Update biased second raw moment estimate)
 mt ←mt/(1−βt
 1) (Compute bias-corrected first moment estimate)
 vt ←vt/(1−βt
 2) (Compute bias-corrected second raw moment estimate)
 θt ←θt−1 −α· bmt/(√b vt +ϵ) (Update parameters)
 end while
 return θt (Resulting parameters)
 In section 2 we describe the algorithm and the properties of its update rule. Section 3 explains
 our initialization bias correction technique, and section 4 provides a theoretical analysis of Adam’s
 convergence in online convex programming. Empirically, our method consistently outperforms other
 methods for a variety of models and datasets, as shown in section 6. Overall, we show that Adam is
 a versatile algorithm that scales to large-scale high-dimensional machine learning problems.
 2 ALGORITHM
 See algorithm 1 for pseudo-code of our proposed algorithm Adam. Let f(θ) be a noisy objec
tive function: a stochastic scalar function that is differentiable w.r.t. parameters θ. We are in
terested in minimizing the expected value of this function, E[f(θ)] w.r.t. its parameters θ. With
 f1(θ), ..., , fT(θ) we denote the realisations of the stochastic function at subsequent timesteps
 1, ..., T. The stochasticity might come from the evaluation at random subsamples (minibatches)
 of datapoints, or arise from inherent function noise. With gt = ∇θft(θ) we denote the gradient, i.e.
 the vector of partial derivatives of ft, w.r.t θ evaluated at timestep t.
 The algorithm updates exponential moving averages of the gradient (mt) and the squared gradient
 (vt) where the hyper-parameters β1,β2 ∈ [0,1) control the exponential decay rates of these moving
 averages. The moving averages themselves are estimates of the 1st moment (the mean) and the
 2nd raw moment (the uncentered variance) of the gradient. However, these moving averages are
 initialized as (vectors of) 0’s, leading to moment estimates that are biased towards zero, especially
 during the initial timesteps, and especially when the decay rates are small (i.e. the βs are close to 1).
 The good news is that this initialization bias can be easily counteracted, resulting in bias-corrected
 estimates b
 mt and b vt. See section 3 for more details.
 Note that the efficiency of algorithm 1 can, at the expense of clarity, be improved upon by changing
 the order of computation, e.g. by replacing the last three lines in the loop with the following lines:
 αt = α·p1−βt
 2/(1−βt
 1) and θt ← θt−1 −αt ·mt/(√vt +ˆ ϵ).
 2.1 ADAM’S UPDATE RULE
 Animportant property of Adam’s update rule is its careful choice of stepsizes. Assuming ϵ = 0, the
 effective step taken in parameter space at timestep t is ∆t = α · b
 mt/√b vt. The effective stepsize has
 two upper bounds: |∆t| ≤ α · (1 − β1)/√1−β2 in the case (1 − β1) > √1−β2, and |∆t| ≤ α
 2
Published as a conference paper at ICLR 2015
 otherwise. The first case only happens in the most severe case of sparsity: when a gradient has
 been zero at all timesteps except at the current timestep. For less sparse cases, the effective stepsize
 will be smaller. When (1 − β1) = √1−β2 we have that |bmt/√b vt| < 1 therefore |∆t| < α. In
 more common scenarios, we will have that b
 mt/√b vt ≈ ±1 since |E[g]/pE[g2]| ≤ 1. The effective
 magnitude of the steps taken in parameter space at each timestep are approximately bounded by
 the stepsize setting α, i.e., |∆t| ⪅ α. This can be understood as establishing a trust region around
 the current parameter value, beyond which the current gradient estimate does not provide sufficient
 information. This typically makes it relatively easy to know the right scale of α in advance. For
 many machine learning models, for instance, we often know in advance that good optima are with
 high probability within some set region in parameter space; it is not uncommon, for example, to
 have a prior distribution over the parameters. Since α sets (an upper bound of) the magnitude of
 steps in parameter space, we can often deduce the right order of magnitude of α such that optima
 can be reached from θ0 within some number of iterations. With a slight abuse of terminology,
 we will call the ratio b
 mt/√b vt the signal-to-noise ratio (SNR). With a smaller SNR the effective
 stepsize ∆t will be closer to zero. This is a desirable property, since a smaller SNR means that
 there is greater uncertainty about whether the direction of b
 mt corresponds to the direction of the true
 gradient. For example, the SNR value typically becomes closer to 0 towards an optimum, leading
 to smaller effective steps in parameter space: a form of automatic annealing. The effective stepsize
 ∆t is also invariant to the scale of the gradients; rescaling the gradients g with factor c will scale b
 mt)/(√c2 · b vt) = bmt/√b vt.
 with a factor c and b vt with a factor c2, which cancel out: (c · b
 3 INITIALIZATION BIAS CORRECTION
 mt
 As explained in section 2, Adam utilizes initialization bias correction terms. We will here derive
 the term for the second moment estimate; the derivation for the first moment estimate is completely
 analogous. Let g be the gradient of the stochastic objective f, and we wish to estimate its second
 raw moment (uncentered variance) using an exponential moving average of the squared gradient,
 with decay rate β2. Let g1,...,gT be the gradients at subsequent timesteps, each a draw from an
 underlying gradient distribution gt ∼ p(gt). Let us initialize the exponential moving average as
 v0 = 0(avector of zeros). First note that the update at timestep t of the exponential moving average
 vt = β2·vt−1 +(1−β2)·g2
 t (where g2
 t indicates the elementwise square gt ⊙gt) can be written as
 a function of the gradients at all previous timesteps:
 t
 X
 vt = (1−β2)
 i=1
 βt−i
 2 ·g2
 i
 (1)
 We wish to know how E[vt], the expected value of the exponential moving average at timestep t,
 relates to the true second moment E[g2
 t], so we can correct for the discrepancy between the two.
 Taking expectations of the left-hand and right-hand sides of eq. (1):
 "
 E[vt] = E
 t
 X
 (1 −β2)
 i=1
 =E[g2
 t] · (1 − β2)
 =E[g2
 t] · (1 − βt
 2) + ζ
 #
 βt−i
 2 ·g2
 i
 t
 X
 i=1
 βt−i
 2 +ζ
 (2)
 (3)
 (4)
 where ζ = 0 if the true second moment E[g2
 i] is stationary; otherwise ζ can be kept small since
 the exponential decay rate β1 can (and should) be chosen such that the exponential moving average
 assigns small weights to gradients too far in the past. What is left is the term (1 − βt
 2) which is
 caused by initializing the running average with zeros. In algorithm 1 we therefore divide by this
 term to correct the initialization bias.
 In case of sparse gradients, for a reliable estimate of the second moment one needs to average over
 many gradients by chosing a small value of β2; however it is exactly this case of small β2 where a
 lack of initialisation bias correction would lead to initial steps that are much larger.
 3
PublishedasaconferencepaperatICLR2015
 4 CONVERGENCEANALYSIS
 WeanalyzetheconvergenceofAdamusingtheonlinelearningframeworkproposedin(Zinkevich,
 2003).Givenanarbitrary,unknownsequenceofconvexcostfunctionsf1(θ),f2(θ),...,fT(θ).At
 eachtimet,ourgoal istopredict theparameterθt andevaluateitonapreviouslyunknowncost
 functionft. Sincethenatureof thesequenceisunknowninadvance,weevaluateouralgorithm
 usingtheregret, that isthesumofall thepreviousdifferencebetweentheonlinepredictionft(θt)
 andthebestfixedpointparameterft(θ∗)fromafeasiblesetXforalltheprevioussteps.Concretely,
 theregretisdefinedas:
 R(T)=
 TX
 t=1
 [ft(θt)−ft(θ∗)] (5)
 whereθ∗=argminθ∈X
 PT
 t=1ft(θ).WeshowAdamhasO(√T)regretboundandaproofisgiven
 intheappendix.Ourresult iscomparabletothebestknownboundforthisgeneralconvexonline
 learningproblem.Wealsousesomedefinitionssimplifyournotation,wheregt≜∇ft(θt)andgt,i
 astheithelement.Wedefineg1:t,i∈Rtasavectorthatcontainstheithdimensionofthegradients
 overall iterations till t, g1:t,i=[g1,i,g2,i,··· ,gt,i]. Also,wedefineγ≜ β2
 1 √β2
 . Our following
 theoremholdswhenthe learningrateαt isdecayingat arateof t−1
 2 andfirstmoment running
 averagecoefficientβ1,tdecayexponentiallywithλ,thatistypicallycloseto1,e.g.1−10−8.
 Theorem4.1. Assumethatthefunctionfthasboundedgradients,∥∇ft(θ)∥2≤G,∥∇ft(θ)∥∞≤
 G∞forallθ∈RdanddistancebetweenanyθtgeneratedbyAdamisbounded,∥θn−θm∥2≤D,
 ∥θm−θn∥∞≤D∞foranym,n∈{1,...,T},andβ1,β2∈[0,1)satisfy β2
 1 √β2
 <1.Letαt= α √t
 andβ1,t=β1λt−1,λ∈(0,1).Adamachievesthefollowingguarantee,forallT≥1.
 R(T)≤ D2
 2α(1−β1)
 d X
 i=1
 p
 Tb vT,i+ α(1+β1)G∞
 (1−β1)√1−β2(1−γ)2
 d X
 i=1
 ∥g1:T,i∥2+
 d X
 i=1
 D2
 ∞G∞
 √1−β2
 2α(1−β1)(1−λ)2
 OurTheorem4.1 implieswhen thedata features are sparse andboundedgradients, the sum
mation termcan bemuch smaller than its upper boundPd
 i=1∥g1:T,i∥2 << dG∞
 √T and Pd
 i=1
 pTb vT,i<<dG∞
 √T,inparticulariftheclassoffunctionanddatafeaturesareintheformof
 section1.2in(Duchietal.,2011).TheirresultsfortheexpectedvalueE[Pd
 i=1∥g1:T,i∥2]alsoapply
 toAdam. Inparticular,theadaptivemethod,suchasAdamandAdagrad,canachieveO(logd√T),
 animprovementoverO(√dT)forthenon-adaptivemethod.Decayingβ1,t towardszeroisimpor
tant inourtheoreticalanalysisandalsomatchespreviousempiricalfindings,e.g. (Sutskeveretal.,
 2013)suggestsreducingthemomentumcoefficientintheendoftrainingcanimproveconvergence.
 Finally,wecanshowtheaverageregretofAdamconverges,
 Corollary4.2. Assumethatthefunctionfthasboundedgradients,∥∇ft(θ)∥2≤G,∥∇ft(θ)∥∞≤
 G∞forallθ∈RdanddistancebetweenanyθtgeneratedbyAdamisbounded,∥θn−θm∥2≤D,
 ∥θm−θn∥∞≤D∞foranym,n∈{1,...,T}. Adamachievesthefollowingguarantee, forall
 T≥1.
 R(T)
 T =O( 1 √T)
 This result canbe obtainedbyusingTheorem4.1 andPd
 i=1∥g1:T,i∥2 ≤ dG∞
 √T. Thus,
 limT→∞
 R(T)
 T =0.
 5 RELATEDWORK
 OptimizationmethodsbearingadirectrelationtoAdamareRMSProp(Tieleman&Hinton,2012;
 Graves,2013)andAdaGrad(Duchietal.,2011); theserelationshipsarediscussedbelow. Other
 stochasticoptimizationmethodsincludevSGD(Schauletal.,2012),AdaDelta(Zeiler,2012)andthe
 naturalNewtonmethodfromRoux&Fitzgibbon(2010),allsettingstepsizesbyestimatingcurvature
 4
Published as a conference paper at ICLR 2015
 from first-order information. The Sum-of-Functions Optimizer (SFO) (Sohl-Dickstein et al., 2014)
 is a quasi-Newton method based on minibatches, but (unlike Adam) has memory requirements linear
 in the number of minibatch partitions of a dataset, which is often infeasible on memory-constrained
 systems such as a GPU. Like natural gradient descent (NGD) (Amari, 1998), Adam employs a
 preconditioner that adapts to the geometry of the data, since b vt is an approximation to the diagonal
 of the Fisher information matrix (Pascanu & Bengio, 2013); however, Adam’s preconditioner (like
 AdaGrad’s) is moreconservative in its adaption than vanilla NGD by preconditioning with the square
 root of the inverse of the diagonal Fisher information matrix approximation.
 RMSProp: An optimization method closely related to Adam is RMSProp (Tieleman & Hinton,
 2012). A version with momentum has sometimes been used (Graves, 2013). There are a few impor
tant differences between RMSProp with momentum and Adam: RMSProp with momentum gener
ates its parameter updates using a momentum on the rescaled gradient, whereas Adam updates are
 directly estimated using a running average of first and second moment of the gradient. RMSProp
 also lacks a bias-correction term; this matters most in case of a value of β2 close to 1 (required in
 case of sparse gradients), since in that case not correcting the bias leads to very large stepsizes and
 often divergence, as we also empirically demonstrate in section 6.4.
 AdaGrad: Analgorithm that works well for sparse gradients is AdaGrad (Duchi et al., 2011). Its
 basic version updates parameters as θt+1 = θt −α·gt/
 qPt
 i=1 g2 t. Note that if we choose β2 to be
 infinitesimally close to 1 from below, then limβ2→1 b vt = t−1 · Pt
 i=1 g2
 t. AdaGrad corresponds to a
 version of Adam with β1 = 0, infinitesimal (1−β2) and a replacement of α by an annealed version
 αt = α·t−1/2, namely θt −α·t−1/2 · bmt/p
 q
 q
 θt − α · gt/
 Pt
 limβ2→1 b vt = θt −α·t−1/2 ·gt/
 t−1 · Pt
 i=1 g2 t =
 i=1 g2 t. Note that this direct correspondence between Adam and Adagrad does
 not hold when removing the bias-correction terms; without bias correction, like in RMSProp, a β2
 infinitesimally close to 1 would lead to infinitely large bias, and infinitely large parameter updates.
 6 EXPERIMENTS
 To empirically evaluate the proposed method, we investigated different popular machine learning
 models, including logistic regression, multilayer fully connected neural networks and deep convolu
tional neural networks. Using large models and datasets, we demonstrate Adam can efficiently solve
 practical deep learning problems.
 We use the same parameter initialization when comparing different optimization algorithms. The
 hyper-parameters, such as learning rate and momentum, are searched over a dense grid and the
 results are reported using the best hyper-parameter setting.
 6.1 EXPERIMENT: LOGISTIC REGRESSION
 Weevaluate ourproposedmethodonL2-regularizedmulti-class logistic regression using the MNIST
 dataset. Logistic regression has a well-studied convex objective, making it suitable for comparison
 of different optimizers without worrying about local minimum issues. The stepsize α in our logistic
 regression experiments is adjusted by 1/√t decay, namely αt = α
 √
 t 
that matches with our theorat
ical prediction from section 4. The logistic regression classifies the class label directly on the 784
 dimension image vectors. We compare Adam to accelerated SGD with Nesterov momentum and
 Adagrad using minibatch size of 128. According to Figure 1, we found that the Adam yields similar
 convergence as SGD with momentum and both converge faster than Adagrad.
 As discussed in (Duchi et al., 2011), Adagrad can efficiently deal with sparse features and gradi
ents as one of its main theoretical results whereas SGD is low at learning rare features. Adam with
 1/√t decay on its stepsize should theoratically match the performance of Adagrad. We examine the
 sparse feature problem using IMDB movie review dataset from (Maas et al., 2011). We pre-process
 the IMDB movie reviews into bag-of-words (BoW) feature vectors including the first 10,000 most
 frequent words. The 10,000 dimension BoW feature vector for each review is highly sparse. As sug
gested in (Wang & Manning, 2013), 50% dropout noise can be applied to the BoW features during
 5
Published as a conference paper at ICLR 2015
 MNIST Logistic Regression
 0.7
 0.6
 0.5
 training cost
 0.4
 0.3
 0.2
 AdaGrad
 SGDNesterov
 Adam
 15
 5
 0
 10
 30
 25
 20
 iterations over entire dataset
 45
 40
 35
 IMDB BoW feature Logistic Regression
 0.50
 0.45
 0.40
 training cost
 0.35
 0.30
 0.25
 0.20
 Adagrad+dropout
 RMSProp+dropout
 SGDNesterov+dropout
 Adam+dropout
 0
 20 40 60 80 100 120 140 160
 iterations over entire dataset
 Figure 1: Logistic regression training negative log likelihood on MNIST images and IMDB movie
 reviews with 10,000 bag-of-words (BoW) feature vectors.
 training to prevent over-fitting. In figure 1, Adagrad outperforms SGD with Nesterov momentum
 by a large margin both with and without dropout noise. Adam converges as fast as Adagrad. The
 empirical performance of Adam is consistent with our theoretical findings in sections 2 and 4. Sim
ilar to Adagrad, Adam can take advantage of sparse features and obtain faster convergence rate than
 normal SGD with momentum.
 6.2 EXPERIMENT: MULTI-LAYER NEURAL NETWORKS
 Multi-layer neural network are powerful models with non-convex objective functions. Although
 our convergence analysis does not apply to non-convex problems, we empirically found that Adam
 often outperforms other methods in such cases. In our experiments, we made model choices that are
 consistent with previous publications in the area; a neural network model with two fully connected
 hidden layers with 1000 hidden units each and ReLU activation are used for this experiment with
 minibatch size of 128.
 First, we study different optimizers using the standard deterministic cross-entropy objective func
tion with L2 weight decay on the parameters to prevent over-fitting. The sum-of-functions (SFO)
 method (Sohl-Dickstein et al., 2014) is a recently proposed quasi-Newton method that works with
 minibatches of data and has shown good performance on optimization of multi-layer neural net
works. We used their implementation and compared with Adam to train such models. Figure 2
 shows that Adam makes faster progress in terms of both the number of iterations and wall-clock
 time. Due to the cost of updating curvature information, SFO is 5-10x slower per iteration com
pared to Adam, and has a memory requirement that is linear in the number minibatches.
 Stochastic regularization methods, such as dropout, are an effective way to prevent over-fitting and
 often used in practice due to their simplicity. SFO assumes deterministic subfunctions, and indeed
 failed to converge on cost functions with stochastic regularization. We compare the effectiveness of
 Adam to other stochastic first order methods on multi-layer neural networks trained with dropout
 noise. Figure 2 shows our results; Adam shows better convergence than other methods.
 6.3 EXPERIMENT: CONVOLUTIONAL NEURAL NETWORKS
 Convolutional neural networks (CNNs) with several layers of convolution, pooling and non-linear
 units have shown considerable success in computer vision tasks. Unlike most fully connected neural
 nets, weight sharing in CNNs results in vastly different gradients in different layers. A smaller
 learning rate for the convolution layers is often used in practice when applying SGD. We show the
 effectiveness of Adam in deep CNNs. Our CNN architecture has three alternating stages of 5x5
 convolution filters and 3x3 max pooling with stride of 2 that are followed by a fully connected layer
 of 1000 rectified linear hidden units (ReLU’s). The input image are pre-processed by whitening, and
 6
PublishedasaconferencepaperatICLR2015
 0 50 100 150 200
 iterations over entire dataset
 10-2
 10-1
 training cost
 MNIST Multilayer Neural Network + dropout
 AdaGrad
 RMSProp
 SGDNesterov
 AdaDelta
 Adam
 (a) (b)
 Figure2: TrainingofmultilayerneuralnetworksonMNISTimages. (a)Neuralnetworksusing
 dropoutstochasticregularization. (b)Neuralnetworkswithdeterministiccostfunction.Wecompare
 withthesum-of-functions(SFO)optimizer(Sohl-Dicksteinetal.,2014)
 0.0 0.5 1.0 1.5 2.0 2.5 3.0
 iterations over entire dataset
 0.5
 1.0
 1.5
 2.0
 2.5
 3.0
 training cost
 CIFAR10 ConvNet First 3 Epoches
 AdaGrad
 AdaGrad+dropout
 SGDNesterov
 SGDNesterov+dropout
 Adam
 Adam+dropout
 0 5 10 15 20 25 30 35 40 45
 iterations over entire dataset
 10-4
 10-3
 10-2
 10-1
 100
 101
 102
 training cost
 CIFAR10 ConvNet
 AdaGrad
 AdaGrad+dropout
 SGDNesterov
 SGDNesterov+dropout
 Adam
 Adam+dropout
 Figure3:Convolutionalneuralnetworkstrainingcost. (left)Trainingcostforthefirstthreeepochs.
 (right)Trainingcostover45epochs.CIFAR-10withc64-c64-c128-1000architecture.
 dropoutnoiseisappliedtotheinputlayerandfullyconnectedlayer.Theminibatchsizeisalsoset
 to128similartopreviousexperiments.
 Interestingly,althoughbothAdamandAdagradmakerapidprogressloweringthecostintheinitial
 stageof thetraining, showninFigure3(left),AdamandSGDeventuallyconvergeconsiderably
 fasterthanAdagradforCNNsshowninFigure3(right).Wenoticethesecondmomentestimateb vt
 vanishestozerosafterafewepochsandisdominatedbytheϵinalgorithm1.Thesecondmoment
 estimateisthereforeapoorapproximationtothegeometryofthecostfunctioninCNNscomparing
 tofullyconnectednetworkfromSection6.2.Whereas, reducingtheminibatchvariancethrough
 thefirstmoment ismoreimportant inCNNsandcontributestothespeed-up.Asaresult,Adagrad
 convergesmuchslower thanothers inthisparticularexperiment. ThoughAdamshowsmarginal
 improvementoverSGDwithmomentum,itadaptslearningratescalefordifferentlayersinsteadof
 handpickingmanuallyasinSGD.
 7
Published as a conference paper at ICLR 2015
 β2=0.99
 β1=0
 β1=0.9
 Loss
 log10(α)
 β2=0.999
 (a) after 10 epochs
 β2=0.9999
 β2=0.99
 β2=0.999
 β2=0.9999
 (b) after 100 epochs
 Figure 4: Effect of bias-correction terms (red line) versus no bias correction terms (green line)
 after 10 epochs (left) and 100 epochs (right) on the loss (y-axes) when learning a Variational Auto
Encoder (VAE) (Kingma & Welling, 2013), for different settings of stepsize α (x-axes) and hyper
parameters β1 and β2.
 6.4 EXPERIMENT: BIAS-CORRECTION TERM
 We also empirically evaluate the effect of the bias correction terms explained in sections 2 and 3.
 Discussed in section 5, removal of the bias correction terms results in a version of RMSProp (Tiele
man & Hinton, 2012) with momentum. We vary the β1 and β2 when training a variational auto
encoder (VAE) with the same architecture as in (Kingma & Welling, 2013) with a single hidden
 layer with 500 hidden units with softplus nonlinearities and a 50-dimensional spherical Gaussian
 latent variable. We iterated over a broad range of hyper-parameter choices, i.e. β1 ∈ [0,0.9] and
 β2 ∈ [0.99,0.999,0.9999], and log10(α) ∈ [−5,...,−1]. Values of β2 close to 1, required for robust
ness to sparse gradients, results in larger initialization bias; therefore we expect the bias correction
 term is important in such cases of slow decay, preventing an adverse effect on optimization.
 In Figure 4, values β2 close to 1 indeed lead to instabilities in training when no bias correction term
 was present, especially at first few epochs of the training. The best results were achieved with small
 values of (1−β2) and bias correction; this was more apparent towards the end of optimization when
 gradients tends to become sparser as hidden units specialize to specific patterns. In summary, Adam
 performed equal or better than RMSProp, regardless of hyper-parameter setting.
 7 EXTENSIONS
 7.1 ADAMAX
 In Adam, the update rule for individual weights is to scale their gradients inversely proportional to a
 (scaled) L2 norm of their individual current and past gradients. We can generalize the L2 norm based
 update rule to a Lp norm based update rule. Such variants become numerically unstable for large
 p. However, in the special case where we let p → ∞, a surprisingly simple and stable algorithm
 emerges; see algorithm 2. We’ll now derive the algorithm. Let, in case of the Lp norm, the stepsize
 at time t be inversely proportional to v1/p
 t
 , where:
 vt = βp
 2vt−1 +(1 −βp
 2)|gt|p
 =(1−βp
 2)
 t
 X
 i=1
 βp(t−i)
 2
 · |gi|p
 (6)
 (7)
 8
Published as a conference paper at ICLR 2015
 Algorithm 2: AdaMax, a variant of Adam based on the infinity norm. See section 7.1 for details.
 Good default settings for the tested machine learning problems are α = 0.002, β1 = 0.9 and
 β2 = 0.999. With βt
 1 we denote β1 to the power t. Here, (α/(1 − βt
 1)) is the learning rate with the
 bias-correction term for the first moment. All operations on vectors are element-wise.
 Require: α: Stepsize
 Require: β1,β2 ∈ [0,1): Exponential decay rates
 Require: f(θ): Stochastic objective function with parameters θ
 Require: θ0: Initial parameter vector
 m0 ←0(Initialize 1st moment vector)
 u0 ←0(Initialize the exponentially weighted infinity norm)
 t ←0(Initialize timestep)
 while θt not converged do
 t ←t+1
 gt ←∇θft(θt−1) (Get gradients w.r.t. stochastic objective at timestep t)
 mt ←β1·mt−1 +(1−β1)·gt (Update biased first moment estimate)
 ut ←max(β2 ·ut−1,|gt|) (Update the exponentially weighted infinity norm)
 θt ←θt−1 −(α/(1−βt
 1))·mt/ut (Update parameters)
 end while
 return θt (Resulting parameters)
 Note that the decay term is here equivalently parameterised as βp
 2 instead of β2. Now let p → ∞,
 and define ut = limp→∞(vt)1/p, then:
 t
 X
 ut = lim
 p→∞
 (vt)1/p = lim
 p→∞
 (1−βp
 2)
 = lim
 i=1
 X
 p→∞
 (1−βp
 2)1/p
 t
 X
 = lim
 p→∞
 i=1
 =max βt−1
 β(t−i)
 2
 i=1
 · |gi|
 2 |g1|,βt−2
 !1/p
 βp(t−i)
 2
 t
 · |gi|p
 βp(t−i)
 2
 · |gi|p
 !1/p
 p
 !1/p
 2 |g2|,...,β2|gt−1|,|gt|
 Which corresponds to the remarkably simple recursive formula:
 ut = max(β2 ·ut−1,|gt|)
 (8)
 (9)
 (10)
 (11)
 (12)
 with initial value u0 = 0. Note that, conveniently enough, we don’t need to correct for initialization
 bias in this case. Also note that the magnitude of parameter updates has a simpler bound with
 AdaMax than Adam, namely: |∆t| ≤ α.
 7.2 TEMPORAL AVERAGING
 Since the last iterate is noisy due to stochastic approximation, better generalization performance is
 often achieved by averaging. Previously in Moulines & Bach (2011), Polyak-Ruppert averaging
 (Polyak & Juditsky, 1992; Ruppert, 1988) has been shown to improve the convergence of standard
 SGD,where ¯ θt = 1
 t 
Pn
 k=1 θk. Alternatively, an exponential moving average over the parameters can
 be used, giving higher weight to more recent parameter values. This can be trivially implemented
 by adding one line to the inner loop of algorithms 1 and 2: ¯ θt ← β2· ¯ θt−1+(1−β2)θt, with ¯ θ0 = 0.
 Initalization bias can again be corrected by the estimator b θt = ¯ θt/(1 − βt
 2).
 8 CONCLUSION
 We have introduced a simple and computationally efficient algorithm for gradient-based optimiza
tion of stochastic objective functions. Our method is aimed towards machine learning problems with
 9
Published as a conference paper at ICLR 2015
 large datasets and/or high-dimensional parameter spaces. The method combines the advantages of
 two recently popular optimization methods: the ability of AdaGrad to deal with sparse gradients,
 and the ability of RMSProp to deal with non-stationary objectives. The method is straightforward
 to implement and requires little memory. The experiments confirm the analysis on the rate of con
vergence in convex problems. Overall, we found Adam to be robust and well-suited to a wide range
 of non-convex optimization problems in the field machine learning.
 9 ACKNOWLEDGMENTS
 This paper would probably not have existed without the support of Google Deepmind. We would
 like to give special thanks to Ivo Danihelka, and Tom Schaul for coining the name Adam. Thanks to
 Kai Fan from Duke University for spotting an error in the original AdaMax derivation. Experiments
 in this work were partly carried out on the Dutch national e-infrastructure with the support of SURF
 Foundation. Diederik Kingma is supported by the Google European Doctorate Fellowship in Deep
 Learning.
 REFERENCES
 Amari, Shun-Ichi. Natural gradient works efficiently in learning. Neural computation, 10(2):251–276, 1998.
 Deng, Li, Li, Jinyu, Huang, Jui-Ting, Yao, Kaisheng, Yu, Dong, Seide, Frank, Seltzer, Michael, Zweig, Geoff,
 He, Xiaodong, Williams, Jason, et al. Recent advances in deep learning for speech research at microsoft.
 ICASSP 2013, 2013.
 Duchi, John, Hazan, Elad, and Singer, Yoram. Adaptive subgradient methods for online learning and stochastic
 optimization. The Journal of Machine Learning Research, 12:2121–2159, 2011.
 Graves, Alex. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013.
 Graves, Alex, Mohamed, Abdel-rahman, and Hinton, Geoffrey. Speech recognition with deep recurrent neural
 networks. In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on,
 pp. 6645–6649. IEEE, 2013.
 Hinton, G.E. and Salakhutdinov, R.R. Reducing the dimensionality of data with neural networks. Science, 313
 (5786):504–507, 2006.
 Hinton, Geoffrey, Deng, Li, Yu, Dong, Dahl, George E, Mohamed, Abdel-rahman, Jaitly, Navdeep, Senior,
 Andrew, Vanhoucke, Vincent, Nguyen, Patrick, Sainath, Tara N, et al. Deep neural networks for acoustic
 modeling in speech recognition: The shared views of four research groups. Signal Processing Magazine,
 IEEE, 29(6):82–97, 2012a.
 Hinton, Geoffrey E, Srivastava, Nitish, Krizhevsky, Alex, Sutskever, Ilya, and Salakhutdinov, Ruslan R. Im
proving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580,
 2012b.
 Kingma, Diederik P and Welling, Max. Auto-Encoding Variational Bayes. In The 2nd International Conference
 on Learning Representations (ICLR), 2013.
 Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. Imagenet classification with deep convolutional
 neural networks. In Advances in neural information processing systems, pp. 1097–1105, 2012.
 Maas, Andrew L, Daly, Raymond E, Pham, Peter T, Huang, Dan, Ng, Andrew Y, and Potts, Christopher.
 Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association
 for Computational Linguistics: Human Language Technologies-Volume 1, pp. 142–150. Association for
 Computational Linguistics, 2011.
 Moulines, Eric and Bach, Francis R. Non-asymptotic analysis of stochastic approximation algorithms for
 machine learning. In Advances in Neural Information Processing Systems, pp. 451–459, 2011.
 Pascanu, Razvan and Bengio, Yoshua. Revisiting natural gradient for deep networks. arXiv preprint
 arXiv:1301.3584, 2013.
 Polyak, Boris T and Juditsky, Anatoli B. Acceleration of stochastic approximation by averaging. SIAM Journal
 on Control and Optimization, 30(4):838–855, 1992.
 10
Published as a conference paper at ICLR 2015
 Roux, Nicolas L and Fitzgibbon, Andrew W. A fast natural newton method. In Proceedings of the 27th
 International Conference on Machine Learning (ICML-10), pp. 623–630, 2010.
 Ruppert, David. Efficient estimations from a slowly convergent robbins-monro process. Technical report,
 Cornell University Operations Research and Industrial Engineering, 1988.
 Schaul, Tom, Zhang, Sixin, and LeCun, Yann. No more pesky learning rates. arXiv preprint arXiv:1206.1106,
 2012.
 Sohl-Dickstein, Jascha, Poole, Ben, and Ganguli, Surya. Fast large-scale optimization by unifying stochas
tic gradient and quasi-newton methods. In Proceedings of the 31st International Conference on Machine
 Learning (ICML-14), pp. 604–612, 2014.
 Sutskever, Ilya, Martens, James, Dahl, George, and Hinton, Geoffrey. On the importance of initialization and
 momentum in deep learning. In Proceedings of the 30th International Conference on Machine Learning
 (ICML-13), pp. 1139–1147, 2013.
 Tieleman, T. and Hinton, G. Lecture 6.5- RMSProp, COURSERA: Neural Networks for Machine Learning.
 Technical report, 2012.
 Wang, Sida and Manning, Christopher. Fast dropout training. In Proceedings of the 30th International Confer
ence on Machine Learning (ICML-13), pp. 118–126, 2013.
 Zeiler, Matthew D. Adadelta: An adaptive learning rate method. arXiv preprint arXiv:1212.5701, 2012.
 Zinkevich, Martin. Online convex programming and generalized infinitesimal gradient ascent. 2003.
 11
PublishedasaconferencepaperatICLR2015
 10 APPENDIX
 10.1 CONVERGENCEPROOF
 Definition10.1. Afunctionf :Rd→Risconvexifforallx,y∈Rd,forallλ∈[0,1],
 λf(x)+(1−λ)f(y)≥f(λx+(1−λ)y)
 Also,noticethataconvexfunctioncanbelowerboundedbyahyperplaneatitstangent.
 Lemma10.2. Ifafunctionf :Rd→Risconvex,thenforallx,y∈Rd,
 f(y)≥f(x)+∇f(x)T(y−x)
 Theabove lemmacanbeusedtoupperboundtheregret andourproof for themaintheoremis
 constructedbysubstitutingthehyperplanewiththeAdamupdaterules.
 Thefollowingtwolemmasareusedtosupportourmaintheorem.Wealsousesomedefinitionssim
plifyournotation,wheregt≜∇ft(θt)andgt,iastheithelement.Wedefineg1:t,i∈Rtasavector
 thatcontainstheithdimensionofthegradientsoveralliterationstillt,g1:t,i=[g1,i,g2,i,··· ,gt,i]
 Lemma10.3. Letgt=∇ft(θt)andg1:tbedefinedasaboveandbounded,∥gt∥2≤G,∥gt∥∞≤
 G∞.Then,
 TX
 t=1
 s
 g2
 t,i
 t ≤2G∞∥g1:T,i∥2
 Proof.WewillprovetheinequalityusinginductionoverT.
 ThebasecaseforT=1,wehave
 q
 g2
 1,i≤2G∞∥g1,i∥2.
 Fortheinductivestep,
 TX
 t=1
 s
 g2
 t,i
 t =
 T−1 X
 t=1
 s
 g2
 t,i
 t +
 s
 g2
 T,i
 T
 ≤2G∞∥g1:T−1,i∥2+
 s
 g2
 T,i
 T
 =2G∞
 q
 ∥g1:T,i∥2
 2−g2
 T+
 s
 g2
 T,i
 T
 From,∥g1:T,i∥2
 2−g2
 T,i+ g4
 T,i
 4∥g1:T,i∥2
 2
 ≥∥g1:T,i∥2
 2−g2
 T,i,wecantakesquarerootofbothsideand
 have,
 q
 ∥g1:T,i∥2
 2−g2
 T,i≤∥g1:T,i∥2− g2
 T,i
 2∥g1:T,i∥2
 ≤∥g1:T,i∥2− g2
 T,i
 2pTG2∞
 Rearrangetheinequalityandsubstitutethe
 q
 ∥g1:T,i∥2
 2−g2
 T,i term,
 G∞
 q
 ∥g1:T,i∥2
 2−g2
 T+
 s
 g2
 T,i
 T ≤2G∞∥g1:T,i∥2
 12
PublishedasaconferencepaperatICLR2015
 Lemma10.4. Letγ≜ β2
 1 √β2
 .Forβ1,β2∈[0,1)thatsatisfy β2
 1 √β2
 <1andboundedgt,∥gt∥2≤G,
 ∥gt∥∞≤G∞,thefollowinginequalityholds
 TX
 t=1
 bm2
 t,i ptb vt,i
 ≤ 2
 1−γ
 1 √1−β2
 ∥g1:T,i∥2
 Proof. Undertheassumption,
 √
 1−βt
 2
 (1−βt
 1)2
 ≤ 1
 (1−β1)2
 .Wecanexpandthelast terminthesummation
 usingtheupdaterulesinAlgorithm1,
 TX
 t=1
 bm2
 t,i ptb vt,i
 =
 T−1 X
 t=1
 bm2
 t,i ptb vt,i
 +
 p
 1−βT
 2
 (1−βT
 1)2
 (PT
 k=1(1−β1)βT−k
 1 gk,i)2
 q
 TPT
 j=1(1−β2)βT−j
 2 g2
 j,i
 ≤
 T−1 X
 t=1
 bm2
 t,i ptb vt,i
 +
 p
 1−βT
 2
 (1−βT
 1)2
 TX
 k=1
 T((1−β1)βT−k
 1 gk,i)2
 q
 TPT
 j=1(1−β2)βT−j
 2 g2
 j,i
 ≤
 T−1 X
 t=1
 bm2
 t,i ptb vt,i
 +
 p
 1−βT
 2
 (1−βT
 1)2
 TX
 k=1
 T((1−β1)βT−k
 1 gk,i)2
 q
 T(1−β2)βT−k
 2 g2
 k,i
 ≤
 T−1 X
 t=1
 bm2
 t,i ptb vt,i
 +
 p
 1−βT
 2
 (1−βT
 1)2
 (1−β1)2
 pT(1−β2)
 TX
 k=1
 T β2
 1 √β2
 T−k
 ∥gk,i∥2
 ≤
 T−1 X
 t=1
 bm2
 t,i ptb vt,i
 + T pT(1−β2)
 TX
 k=1
 γT−k∥gk,i∥2
 Similarly,wecanupperboundtherestofthetermsinthesummation.
 TX
 t=1
 bm2
 t,i ptb vt,i
 ≤
 TX
 t=1
 ∥gt,i∥2 pt(1−β2)
 T−t X
 j=0
 tγj
 ≤
 TX
 t=1
 ∥gt,i∥2 pt(1−β2)
 TX
 j=0
 tγj
 Forγ<1,usingtheupperboundonthearithmetic-geometricseries,P
 ttγt< 1
 (1−γ)2
 :
 TX
 t=1
 ∥gt,i∥2 pt(1−β2)
 TX
 j=0
 tγj≤ 1
 (1−γ)2
 √1−β2
 TX
 t=1
 ∥gt,i∥2 √t
 ApplyLemma10.3,
 TX
 t=1
 bm2
 t,i ptb vt,i
 ≤ 2G∞
 (1−γ)2
 √1−β2
 ∥g1:T,i∥2
 Tosimplifythenotation,wedefineγ≜ β2
 1 √β2
 . Intuitively,ourfollowingtheoremholdswhenthe
 learningrateαt isdecayingatarateoft−1
 2 andfirstmomentrunningaveragecoefficientβ1,tdecay
 exponentiallywithλ,thatistypicallycloseto1,e.g.1−10−8.
 Theorem10.5.Assumethatthefunctionfthasboundedgradients,∥∇ft(θ)∥2≤G,∥∇ft(θ)∥∞≤
 G∞forallθ∈RdanddistancebetweenanyθtgeneratedbyAdamisbounded,∥θn−θm∥2≤D,
 13
PublishedasaconferencepaperatICLR2015
 ∥θm−θn∥∞≤D∞foranym,n∈{1,...,T},andβ1,β2∈[0,1)satisfy β2
 1 √β2
 <1.Letαt= α √t
 andβ1,t=β1λt−1,λ∈(0,1).Adamachievesthefollowingguarantee,forallT≥1.
 R(T)≤ D2
 2α(1−β1)
 d X
 i=1
 p
 Tb vT,i+ α(β1+1)G∞
 (1−β1)√1−β2(1−γ)2
 d X
 i=1
 ∥g1:T,i∥2+
 d X
 i=1
 D2
 ∞G∞
 √1−β2
 2α(1−β1)(1−λ)2
 Proof. UsingLemma10.2,wehave,
 ft(θt)−ft(θ∗)≤gT
 t (θt−θ∗)=
 d X
 i=1
 gt,i(θt,i−θ∗
 ,i)
 Fromtheupdaterulespresentedinalgorithm1,
 θt+1=θt−αtbmt/
 p
 b vt
 =θt− αt
 1−βt
 1
 β1,t √b vt
 mt−1+ (1−β1,t) √b vt
 gt
 Wefocusontheithdimensionoftheparametervectorθt∈Rd. Subtract thescalarθ∗
 ,i andsquare
 bothsidesoftheaboveupdaterule,wehave,
 (θt+1,i−θ∗
 ,i)2=(θt,i−θ∗
 ,i)2− 2αt
 1−βt
 1
 ( β1,t pb vt,i
 mt−1,i+(1− β1,t) pb vt,i
 gt,i)(θt,i−θ∗
 ,i)+α2
 t( bmt,i pb vt,i
 )2
 WecanrearrangetheaboveequationanduseYoung’sinequality,ab≤a2/2+b2/2.Also,itcanbe
 shownthatpb vt,i=
 qPt
 j=1(1−β2)βt−j
 2 g2
 j,i/p1−βt
 2≤∥g1:t,i∥2andβ1,t≤β1.Then
 gt,i(θt,i−θ∗
 ,i)=(1−βt
 1)pb vt,i
 2αt(1−β1,t) (θt,i−θ∗
 ,t)2−(θt+1,i−θ∗
 ,i)2
 + β1,t
 (1−β1,t)
 b v1
 4
 t−1,i √αt−1
 (θ∗
 ,i−θt,i)√αt−1
 mt−1,i
 b v1
 4
 t−1,i
 +αt(1−βt
 1)pb vt,i
 2(1−β1,t) ( bmt,i pb vt,i
 )2
 ≤ 1
 2αt(1−β1) (θt,i−θ∗
 ,t)2−(θt+1,i−θ∗
 ,i)2
 p
 b vt,i+ β1,t
 2αt−1(1−β1,t) (θ∗
 ,i−θt,i)2
 p
 b vt−1,i
 + β1αt−1
 2(1−β1)
 m2
 t−1,i pb vt−1,i
 + αt
 2(1−β1)
 bm2
 t,i pb vt,i
 WeapplyLemma10.4totheaboveinequalityandderivetheregretboundbysummingacrossall
 thedimensionsfori∈1,...,dintheupperboundofft(θt)−ft(θ∗)andthesequenceofconvex
 functionsfort∈1,...,T:
 R(T)≤
 d X
 i=1
 1
 2α1(1−β1) (θ1,i−θ∗
 ,i)2
 p
 b v1,i+
 d X
 i=1
 TX
 t=2
 1
 2(1−β1) (θt,i−θ∗
 ,i)2(
 pb vt,i
 αt
 −
 pb vt−1,i
 αt−1
 )
 + β1αG∞
 (1−β1)√1−β2(1−γ)2
 d X
 i=1
 ∥g1:T,i∥2+ αG∞
 (1−β1)√1−β2(1−γ)2
 d X
 i=1
 ∥g1:T,i∥2
 +
 d X
 i=1
 TX
 t=1
 β1,t
 2αt(1−β1,t) (θ∗
 ,i−θt,i)2
 p
 b vt,i
 14
PublishedasaconferencepaperatICLR2015
 Fromtheassumption,∥θt−θ∗∥2≤D,∥θm−θn∥∞≤D∞,wehave:
 R(T)≤ D2
 2α(1−β1)
 d X
 i=1
 p
 Tb vT,i+ α(1+β1)G∞
 (1−β1)√1−β2(1−γ)2
 d X
 i=1
 ∥g1:T,i∥2+D2
 ∞
 2α
 d X
 i=1
 t X
 t=1
 β1,t
 (1−β1,t)
 p
 tb vt,i
 ≤ D2
 2α(1−β1)
 d X
 i=1
 p
 Tb vT,i+ α(1+β1)G∞
 (1−β1)√1−β2(1−γ)2
 d X
 i=1
 ∥g1:T,i∥2
 +D2
 ∞G∞
 √1−β2
 2α
 d X
 i=1
 t X
 t=1
 β1,t
 (1−β1,t)
 √t
 Wecanusearithmeticgeometricseriesupperboundforthelastterm:
 t X
 t=1
 β1,t
 (1−β1,t)
 √t≤
 t X
 t=1
 1
 (1−β1)λt−1
 √t
 ≤
 t X
 t=1
 1
 (1−β1)λt−1t
 ≤ 1
 (1−β1)(1−λ)2
 Therefore,wehavethefollowingregretbound:
 R(T)≤ D2
 2α(1−β1)
 d X
 i=1
 p
 Tb vT,i+ α(1+β1)G∞
 (1−β1)√1−β2(1−γ)2
 d X
 i=1
 ∥g1:T,i∥2+
 d X
 i=1
 D2
 ∞G∞
 √1−β2
 2αβ1(1−λ)2
 15