High-Resolution Image Synthesis with Latent Diffusion Models
 Robin Rombach1 *
 Andreas Blattmann1 
Dominik Lorenz1
 Patrick Esser Bj¨ orn Ommer1
 1Ludwig Maximilian University of Munich & IWR, Heidelberg University, Germany Runway ML
 https://github.com/CompVis/latent-diffusion
 Abstract
 arXiv:2112.10752v2  [cs.CV]  13 Apr 2022
 By decomposing the image formation process into a se
quential application of denoising autoencoders, diffusion
 models (DMs) achieve state-of-the-art synthesis results on
 image data and beyond. Additionally, their formulation al
lows for a guiding mechanism to control the image gen
eration process without retraining. However, since these
 models typically operate directly in pixel space, optimiza
tion of powerful DMs often consumes hundreds of GPU
 days and inference is expensive due to sequential evalu
ations. To enable DM training on limited computational
 resources while retaining their quality and flexibility, we
 apply them in the latent space of powerful pretrained au
toencoders. In contrast to previous work, training diffusion
 models on such a representation allows for the first time
 to reach a near-optimal point between complexity reduc
tion and detail preservation, greatly boosting visual fidelity.
 By introducing cross-attention layers into the model archi
tecture, we turn diffusion models into powerful and flexi
ble generators for general conditioning inputs such as text
 or bounding boxes and high-resolution synthesis becomes
 possible in a convolutional manner. Our latent diffusion
 models (LDMs) achieve new state-of-the-art scores for im
age inpainting and class-conditional image synthesis and
 highly competitive performance on various tasks, includ
ing text-to-image synthesis, unconditional image generation
 and super-resolution, while significantly reducing computa
tional requirements compared to pixel-based DMs.
 1. Introduction
 Image synthesis is one of the computer vision fields with
 the most spectacular recent development, but also among
 those with the greatest computational demands. Espe
cially high-resolution synthesis of complex, natural scenes
 is presently dominated by scaling up likelihood-based mod
els, potentially containing billions of parameters in autore
gressive (AR) transformers [66,67]. In contrast, the promis
ing results of GANs [3, 27, 40] have been revealed to be
 mostly confined to data with comparably limited variability
 as their adversarial learning procedure does not easily scale
 to modeling complex, multi-modal distributions. Recently,
 diffusion models [82], which are built from a hierarchy of
 denoising autoencoders, have shown to achieve impressive
 *The first two authors contributed equally to this work.
 ours (f = 4)
 Input
 PSNR: 274 R-FID: 058
 DALL-E (f = 8)
 PSNR: 228 R-FID: 3201
 VQGAN(f =16)
 PSNR: 199 R-FID: 498
 Figure 1. Boosting the upper bound on achievable quality with
 less agressive downsampling. Since diffusion models offer excel
lent inductive biases for spatial data, we do not need the heavy spa
tial downsampling of related generative models in latent space, but
 can still greatly reduce the dimensionality of the data via suitable
 autoencoding models, see Sec. 3. Images are from the DIV2K [1]
 validation set, evaluated at 5122 px. We denote the spatial down
sampling factor by f. Reconstruction FIDs [29] and PSNR are
 calculated on ImageNet-val. [12]; see also Tab. 8.
 results in image synthesis [30,85] and beyond [7,45,48,57],
 and define the state-of-the-art in class-conditional image
 synthesis [15,31] and super-resolution [72]. Moreover, even
 unconditional DMs can readily be applied to tasks such
 as inpainting and colorization [85] or stroke-based syn
thesis [53], in contrast to other types of generative mod
els [19,46,69]. Being likelihood-based models, they do not
 exhibit mode-collapse and training instabilities as GANs
 and, by heavily exploiting parameter sharing, they can
 model highly complex distributions of natural images with
out involving billions of parameters as in AR models [67].
 Democratizing High-Resolution Image Synthesis DMs
 belong to the class of likelihood-based models, whose
 mode-covering behavior makes them prone to spend ex
cessive amounts of capacity (and thus compute resources)
 on modeling imperceptible details of the data [16,73]. Al
though the reweighted variational objective [30] aims to ad
dress this by undersampling the initial denoising steps, DMs
 are still computationally demanding, since training and
 evaluating such a model requires repeated function evalu
ations (and gradient computations) in the high-dimensional
 space of RGB images. As an example, training the most
 powerful DMsoften takes hundreds of GPU days (e.g. 150
1000V100daysin[15])andrepeatedevaluations on anoisy
 version of the input space render also inference expensive,
 1
so that producing 50k samples takes approximately 5 days
 [15] on a single A100 GPU. This has two consequences for
 the research community and users in general: Firstly, train
ing such a model requires massive computational resources
 only available to a small fraction of the field, and leaves a
 huge carbon footprint [65,86]. Secondly, evaluating an al
ready trained model is also expensive in time and memory,
 since the same model architecture must run sequentially for
 a large number of steps (e.g. 25- 1000 steps in [15]).
 To increase the accessibility of this powerful model class
 and at the same time reduce its significant resource con
sumption, a method is needed that reduces the computa
tional complexity for both training and sampling. Reducing
 the computational demands of DMs without impairing their
 performance is, therefore, key to enhance their accessibility.
 Departure to Latent Space Our approach starts with
 the analysis of already trained diffusion models in pixel
 space: Fig. 2 shows the rate-distortion trade-off of a trained
 model. As with any likelihood-based model, learning can
 be roughly divided into two stages: First is a perceptual
 compression stage which removes high-frequency details
 but still learns little semantic variation. In the second stage,
 the actual generative model learns the semantic and concep
tual composition of the data (semantic compression). We
 thus aim to first find a perceptually equivalent, but compu
tationally more suitable space, in which we will train diffu
sion models for high-resolution image synthesis.
 Following common practice [11,23,66,67,96], we sep
arate training into two distinct phases: First, we train
 an autoencoder which provides a lower-dimensional (and
 thereby efficient) representational space which is perceptu
ally equivalent to the data space. Importantly, and in con
trast to previous work [23,66], we do not need to rely on ex
cessive spatial compression, as we train DMs in the learned
 latent space, which exhibits better scaling properties with
 respect to the spatial dimensionality. The reduced complex
ity also provides efficient image generation from the latent
 space with a single network pass. We dub the resulting
 model class Latent Diffusion Models (LDMs).
 Anotable advantage of this approach is that we need to
 train the universal autoencoding stage only once and can
 therefore reuse it for multiple DM trainings or to explore
 possibly completely different tasks [81]. This enables effi
cient exploration of a large number of diffusion models for
 various image-to-image and text-to-image tasks. For the lat
ter, we design an architecture that connects transformers to
 the DM’s UNet backbone [71] and enables arbitrary types
 of token-based conditioning mechanisms, see Sec. 3.3.
 In sum, our work makes the following contributions:
 (i) In contrast to purely transformer-based approaches
 [23,66], our method scales more graceful to higher dimen
sional data and can thus (a) work on a compression level
 which provides more faithful and detailed reconstructions
 than previous work (see Fig. 1) and (b) can be efficiently
 Figure 2. Illustrating perceptual and semantic compression: Most
 bits of a digital image correspond to imperceptible details. While
 DMsallow to suppress this semantically meaningless information
 by minimizing the responsible loss term, gradients (during train
ing) and the neural network backbone (training and inference) still
 need to be evaluated on all pixels, leading to superfluous compu
tations and unnecessarily expensive optimization and inference.
 We propose latent diffusion models (LDMs) as an effective gener
ative model and a separate mild compression stage that only elim
inates imperceptible details. Data and images from [30].
 applied to high-resolution synthesis of megapixel images.
 (ii) We achieve competitive performance on multiple
 tasks (unconditional image synthesis, inpainting, stochastic
 super-resolution) and datasets while significantly lowering
 computational costs. Compared to pixel-based diffusion ap
proaches, we also significantly decrease inference costs.
 (iii) We show that, in contrast to previous work [93]
 which learns both an encoder/decoder architecture and a
 score-based prior simultaneously, our approach does not re
quire a delicate weighting of reconstruction and generative
 abilities. This ensures extremely faithful reconstructions
 and requires very little regularization of the latent space.
 (iv) We find that for densely conditioned tasks such
 as super-resolution, inpainting and semantic synthesis, our
 model can be applied in a convolutional fashion and render
 large, consistent images of 10242 px.
 (v) Moreover, we design a general-purpose conditioning
 mechanism based on cross-attention, enabling multi-modal
 training. We use it to train class-conditional, text-to-image
 and layout-to-image models.
 (vi) Finally, we release pretrained latent diffusion
 and autoencoding models at https://github.
 com/CompVis/latent-diffusion which might be
 reusable for a various tasks besides training of DMs [81].
 2. Related Work
 Generative Models for Image Synthesis The high di
mensional nature of images presents distinct challenges
 to generative modeling. Generative Adversarial Networks
 (GAN) [27] allow for efficient sampling of high resolution
 images with good perceptual quality [3, 42], but are diffi
2
cult to optimize [2,28,54] and struggle to capture the full
 data distribution [55]. In contrast, likelihood-based meth
ods emphasize good density estimation which renders op
timization more well-behaved. Variational autoencoders
 (VAE) [46] and flow-based models [18,19] enable efficient
 synthesis of high resolution images [9, 44, 92], but sam
ple quality is not on par with GANs. While autoregressive
 models (ARM) [6,10,94,95] achieve strong performance
 in density estimation, computationally demanding architec
tures [97] and a sequential sampling process limit them to
 low resolution images. Because pixel based representations
 of images contain barely perceptible, high-frequency de
tails [16,73], maximum-likelihood training spends a dispro
portionate amount of capacity on modeling them, resulting
 in long training times. To scale to higher resolutions, several
 two-stage approaches [23,67,101,103] use ARMs to model
 a compressed latent image space instead of raw pixels.
 Recently, Diffusion Probabilistic Models (DM) [82],
 have achieved state-of-the-art results in density estimation
 [45] as well as in sample quality [15]. The generative power
 of these models stems from a natural fit to the inductive bi
ases of image-like data when their underlying neural back
bone is implemented as a UNet [15,30,71,85]. The best
 synthesis quality is usually achieved when a reweighted ob
jective [30] is used for training. In this case, the DM corre
sponds to a lossy compressor and allow to trade image qual
ity for compression capabilities. Evaluating and optimizing
 these models in pixel space, however, has the downside of
 low inference speed and very high training costs. While
 the former can be partially adressed by advanced sampling
 strategies [47,75,84] and hierarchical approaches [31,93],
 training on high-resolution image data always requires to
 calculate expensive gradients. We adress both drawbacks
 with our proposed LDMs, which work on a compressed la
tent space of lower dimensionality. This renders training
 computationally cheaper and speeds up inference with al
most no reduction in synthesis quality (see Fig. 1).
 Two-Stage Image Synthesis To mitigate the shortcom
ings of individual generative approaches, a lot of research
 [11, 23, 67, 70, 101, 103] has gone into combining the
 strengths of different methods into more efficient and per
formant models via a two stage approach. VQ-VAEs [67,
 101] use autoregressive models to learn an expressive prior
 over a discretized latent space. [66] extend this approach to
 text-to-image generation by learning a joint distributation
 over discretized image and text representations. More gen
erally, [70] uses conditionally invertible networks to pro
vide a generic transfer between latent spaces of diverse do
mains. Different from VQ-VAEs, VQGANs [23,103] em
ploy a first stage with an adversarial and perceptual objec
tive to scale autoregressive transformers to larger images.
 However, the high compression rates required for feasible
 ARM training, which introduces billions of trainable pa
rameters [23,66], limit the overall performance of such ap
proaches and less compression comes at the price of high
 computational cost [23,66]. Our work prevents such trade
offs, as our proposed LDMs scale more gently to higher
 dimensional latent spaces due to their convolutional back
bone. Thus, we are free to choose the level of compression
 which optimally mediates between learning a powerful first
 stage, without leaving too much perceptual compression up
 to the generative diffusion model while guaranteeing high
f
 idelity reconstructions (see Fig. 1).
 While approaches to jointly [93] or separately [80] learn
 an encoding/decoding model together with a score-based
 prior exist, the former still require a difficult weighting be
tween reconstruction and generative capabilities [11] and
 are outperformed by our approach (Sec. 4), and the latter
 focus on highly structured images such as human faces.
 3. Method
 To lower the computational demands of training diffu
sion models towards high-resolution image synthesis, we
 observe that although diffusion models allow to ignore
 perceptually irrelevant details by undersampling the corre
sponding loss terms [30], they still require costly function
 evaluations in pixel space, which causes huge demands in
 computation time and energy resources.
 We propose to circumvent this drawback by introducing
 an explicit separation of the compressive from the genera
tive learning phase (see Fig. 2). To achieve this, we utilize
 an autoencoding model which learns a space that is percep
tually equivalent to the image space, but offers significantly
 reduced computational complexity.
 Such an approach offers several advantages: (i) By leav
ing the high-dimensional image space, we obtain DMs
 which are computationally much more efficient because
 sampling is performed on a low-dimensional space. (ii) We
 exploit the inductive bias of DMs inherited from their UNet
 architecture [71], which makes them particularly effective
 for data with spatial structure and therefore alleviates the
 need for aggressive, quality-reducing compression levels as
 required by previous approaches [23,66]. (iii) Finally, we
 obtain general-purpose compression models whose latent
 space can be used to train multiple generative models and
 which can also be utilized for other downstream applica
tions such as single-image CLIP-guided synthesis [25].
 3.1. Perceptual Image Compression
 Our perceptual compression model is based on previous
 work [23] and consists of an autoencoder trained by com
bination of a perceptual loss [106] and a patch-based [33]
 adversarial objective [20,23,103]. This ensures that the re
constructions are confined to the image manifold by enforc
ing local realism and avoids bluriness introduced by relying
 solely on pixel-space losses such as L2 or L1 objectives.
 More precisely, given an image x RHW 3 in RGB
 space, the encoder E encodes x into a latent representa
3
tion z = E(x), and the decoder D reconstructs the im
age from the latent, giving x = D(z) = D(E(x)), where
 z 
Rh w c. Importantly, the encoder downsamples the
 image by a factor f = H h = W w, and we investigate
 different downsampling factors f = 2m, with m N.
 In order to avoid arbitrarily high-variance latent spaces,
 we experiment with two different kinds of regularizations.
 The first variant, KL-reg., imposes a slight KL-penalty to
wards a standard normal on the learned latent, similar to a
 VAE [46,69], whereas VQ-reg. uses a vector quantization
 layer [96] within the decoder. This model can be interpreted
 as a VQGAN[23] but with the quantization layer absorbed
 by the decoder. Because our subsequent DM is designed
 to work with the two-dimensional structure of our learned
 latent space z = E(x), we can use relatively mild compres
sion rates and achieve very good reconstructions. This is
 in contrast to previous works [23, 66], which relied on an
 arbitrary 1D ordering of the learned space z to model its
 distribution autoregressively and thereby ignored much of
 the inherent structure of z. Hence, our compression model
 preserves details of x better (see Tab. 8). The full objective
 and training details can be found in the supplement.
 3.2. Latent Diffusion Models
 Diffusion Models [82] are probabilistic models designed to
 learn a data distribution p(x) by gradually denoising a nor
mally distributed variable, which corresponds to learning
 the reverse process of a fixed Markov Chain of length T.
 For image synthesis, the most successful models [15,30,72]
 rely on a reweighted variant of the variational lower bound
 on p(x), which mirrors denoising score-matching [85].
 These models can be interpreted as an equally weighted
 sequence of denoising autoencoders (xt t); t = 1 T,
 which are trained to predict a denoised variant of their input
 xt, where xt is a noisy version of the input x. The corre
sponding objective can be simplified to (Sec. B)
 LDM =Ex N(01) t
 with t uniformly sampled from 1
 (xt t) 2
 2
 T .
 (1)
 Generative Modeling of Latent Representations With
 our trained perceptual compression models consisting of E
 and D, wenowhaveaccess to an efficient, low-dimensional
 latent space in which high-frequency, imperceptible details
 are abstracted away. Compared to the high-dimensional
 pixel space, this space is more suitable for likelihood-based
 generative models, as they can now (i) focus on the impor
tant, semantic bits of the data and (ii) train in a lower di
mensional, computationally much more efficient space.
 Unlike previous work that relied on autoregressive,
 attention-based transformer models in a highly compressed,
 discrete latent space [23,66,103], we can take advantage of
 image-specific inductive biases that our model offers. This
 Latent Space
 Diffusion Process
 Denoising U-Net
 Pixel Space
 Conditioning 
Semantic 
Map
 Text
 Repres 
entations
 Images
 denoising step
 crossattention
 switch
 skip connection
 concat
 Figure 3. We condition LDMs either via concatenation or by a
 more general cross-attention mechanism. See Sec. 3.3
 includes the ability to build the underlying UNet primar
ily from 2D convolutional layers, and further focusing the
 objective on the perceptually most relevant bits using the
 reweighted bound, which now reads
 LLDM := EE(x) N(01)t
 (zt t) 2
 2
 (2)
 The neural backbone ( t) of our model is realized as a
 time-conditional UNet [71]. Since the forward process is
 f
 ixed, zt can be efficiently obtained from E during training,
 and samples from p(z) can be decoded to image space with
 a single pass through D.
 3.3. Conditioning Mechanisms
 Similar to other types of generative models [56, 83],
 diffusion models are in principle capable of modeling
 conditional distributions of the form p(zy). This can
 be implemented with a conditional denoising autoencoder
 (zt t y) and paves the way to controlling the synthesis
 process through inputs y such as text [68], semantic maps
 [33,61] or other image-to-image translation tasks [34].
 In the context of image synthesis, however, combining
 the generative power of DMs with other types of condition
ings beyond class-labels [15] or blurred variants of the input
 image [72] is so far an under-explored area of research.
 WeturnDMsintomoreflexibleconditionalimagegener
ators by augmenting their underlying UNet backbone with
 the cross-attention mechanism [97], which is effective for
 learning attention-based models of various input modali
ties [35,36]. To pre-process y from various modalities (such
 as language prompts) we introduce a domain specific en
coder 
that projects y to an intermediate representation
 (y) RM d ,whichisthen mapped to the intermediate
 layers of the UNet via a cross-attention layer implementing
 Attention(QKV ) = softmax QKT
 d
 V,with
 Q=W(i)
 Q i(zt) K =W(i)
 K (y) V =W(i)
 V (y)
 Here, i(zt) 
RN di denotes a (flattened) intermediate
 representation of the UNet implementing 
4
 and W(i)
 V
CelebAHQ
 FFHQ
 LSUN-Churches
 LSUN-Beds
 ImageNet
 Figure 4. Samples from LDMs trained on CelebAHQ [39], FFHQ [41], LSUN-Churches [102], LSUN-Bedrooms [102] and class
conditional ImageNet [12], each with a resolution of 256 256. Best viewed when zoomed in. For more samples cf. the supplement.
 Rd di , W(i)
 Q Rd d &W(i)
 K Rd d arelearnablepro
jection matrices [36,97]. See Fig. 3 for a visual depiction.
 Based on image-conditioning pairs, we then learn the
 conditional LDM via
 LLDM := EE(x)y N(01) t
 (zt t
 (y)) 2
 2
 (3)
 where both and are jointly optimized via Eq. 3. This
 conditioning mechanism is flexible as can be parameter
ized with domain-specific experts, e.g. (unmasked) trans
formers [97] when y are text prompts (see Sec. 4.3.1)
 4. Experiments
 LDMs provide means to flexible and computationally
 tractable diffusion based image synthesis of various image
 modalities, which we empirically show in the following.
 Firstly, however, we analyze the gains of our models com
pared to pixel-based diffusion models in both training and
 inference. Interestingly, we find that LDMs trained in VQ
regularized latent spaces sometimes achieve better sample
 quality, even though the reconstruction capabilities of VQ
regularized first stage models slightly fall behind those of
 their continuous counterparts, cf. Tab. 8. A visual compari
son between the effects of first stage regularization schemes
 on LDMtraining and their generalization abilities to resolu
tions > 2562 can be found in Appendix D.1. In E.2 we list
 details on architecture, implementation, training and evalu
ation for all results presented in this section.
 4.1. On Perceptual Compression Tradeoffs
 This section analyzes the behavior of our LDMs with dif
ferent downsampling factors f 
12481632 (abbre
viated as LDM-f, where LDM-1 corresponds to pixel-based
 DMs). To obtain a comparable test-field, we fix the com
putational resources to a single NVIDIA A100 for all ex
periments in this section and train all models for the same
 number of steps and with the same number of parameters.
 Tab. 8 shows hyperparameters and reconstruction perfor
mance of the first stage models used for the LDMs com
pared in this section. Fig. 6 shows sample quality as a func
tion of training progress for 2M steps of class-conditional
 models on the ImageNet [12] dataset. We see that, i) small
 downsampling factors for LDM-1,2 result in slow train
ing progress, whereas ii) overly large values of f cause stag
nating fidelity after comparably few training steps. Revis
iting the analysis above (Fig. 1 and 2) we attribute this to
 i) leaving most of perceptual compression to the diffusion
 model and ii) too strong first stage compression resulting
 in information loss and thus limiting the achievable qual
ity. LDM-4-16 strike a good balance between efficiency
 and perceptually faithful results, which manifests in a sig
nificant FID [29] gap of 38 between pixel-based diffusion
 (LDM-1) and LDM-8 after 2M training steps.
 In Fig. 7, we compare models trained on CelebA
HQ [39] and ImageNet in terms sampling speed for differ
ent numbers of denoising steps with the DDIM sampler [84]
 and plot it against FID-scores [29]. LDM-4-8 outper
form models with unsuitable ratios of perceptual and con
ceptual compression. Especially compared to pixel-based
 LDM-1, they achieve much lower FID scores while simulta
neously significantly increasing sample throughput. Com
plex datasets such as ImageNet require reduced compres
sion rates to avoid reducing quality. In summary, LDM-4
 and-8 offer the best conditions for achieving high-quality
 synthesis results.
 4.2. Image Generation with Latent Diffusion
 We train unconditional models of 2562 images on
 CelebA-HQ [39], FFHQ [41], LSUN-Churches and-Bedrooms [102] and evaluate the i) sample quality and ii)
 their coverage of the data manifold using ii) FID [29] and
 ii) Precision-and-Recall [50]. Tab. 1 summarizes our re
sults. On CelebA-HQ, we report a new state-of-the-art FID
 of 511, outperforming previous likelihood-based models as
 well as GANs. We also outperform LSGM [93] where a la
tent diffusion model is trained jointly together with the first
 stage. In contrast, we train diffusion models in a fixed space
 5
Text-to-ImageSynthesisonLAION.1.45BModel.
 ’Astreetsignthatreads
 “LatentDiffusion”’
 ’Azombieinthe
 styleofPicasso’
 ’Animageofananimal
 halfmousehalfoctopus’
 ’Anillustrationofaslightly
 consciousneuralnetwork’
 ’Apaintingofa
 squirreleatingaburger’
 ’Awatercolorpaintingofa
 chairthatlookslikeanoctopus’
 ’Ashirtwiththeinscription:
 “Ilovegenerativemodels!”’
 Figure5. Samplesforuser-definedtextpromptsfromourmodel for text-to-imagesynthesis,LDM-8(KL),whichwas trainedonthe
 LAION[78]database.Samplesgeneratedwith200DDIMstepsand =10.Weuseunconditionalguidance[32]withs=100.
 Figure6.Analyzingthetrainingofclass-conditionalLDMswith
 differentdownsamplingfactorsfover2MtrainstepsontheIm
ageNetdataset. Pixel-basedLDM-1requiressubstantiallylarger
 traintimescomparedtomodelswithlargerdownsamplingfactors
 (LDM-4-16 ).ToomuchperceptualcompressionasinLDM-32
 limitstheoverallsamplequality.Allmodelsaretrainedonasin
gleNVIDIAA100withthesamecomputationalbudget. Results
 obtainedwith100DDIMsteps[84]and =0.
 Figure7. ComparingLDMswithvaryingcompressionon the
 CelebA-HQ(left)andImageNet(right)datasets.Differentmark
ers indicate 102050100200 samplingstepsusingDDIM,
 fromrighttoleftalongeachline.ThedashedlineshowstheFID
 scoresfor200steps, indicatingthestrongperformanceofLDM
4-8 . FIDscoresassessedon5000samples. Allmodelswere
 trainedfor500k(CelebA)/2M(ImageNet)stepsonanA100.
 andavoidthedifficultyofweighingreconstructionquality
 againstlearningtheprioroverthelatentspace,seeFig.1-2.
 Weoutperformpriordiffusionbasedapproachesonall
 but theLSUN-Bedroomsdataset,whereourscoreisclose
 toADM[15],despiteutilizinghalf itsparametersandre
quiring4-times less trainresources(seeAppendixE.3.5).
 CelebA-HQ256 256 FFHQ256 256
 Method FID Prec. Recall Method FID Prec. Recall
 DC-VAE[63] 15.8-- ImageBART[21] 9.57-
VQGAN+T.[23](k=400) 10.2-- U-NetGAN(+aug)[77] 10.9(7.6)-
PGGAN[39] 8.0-- UDM[43] 5.54-
LSGM[93] 7.22-- StyleGAN[41] 4.16 0.71 0.46
 UDM[43] 7.16-- ProjectedGAN[76] 3.08 0.65 0.46
 LDM-4(ours,500-s ) 5.11 0.72 0.49 LDM-4(ours,200-s) 4.98 0.73 0.50
 LSUN-Churches256 256 LSUN-Bedrooms256 256
 Method FID Prec. Recall Method FID Prec. Recall
 DDPM[30] 7.89-- ImageBART[21] 5.51-
ImageBART[21] 7.32-- DDPM[30] 4.9-
PGGAN[39] 6.42-- UDM[43] 4.57-
StyleGAN[41] 4.21-- StyleGAN[41] 2.35 0.59 0.48
 StyleGAN2[42] 3.86-- ADM[15] 1.90 0.66 0.51
 ProjectedGAN[76] 1.59 0.61 0.44 ProjectedGAN[76] 1.52 0.61 0.34
 LDM-8 (ours,200-s) 4.02 0.64 0.52 LDM-4(ours,200-s) 2.95 0.66 0.48
 Table1. Evaluationmetricsforunconditional imagesynthesis.
 CelebA-HQresults reproduced from[43,63,100], FFHQfrom
 [42,43]. :N-sreferstoNsamplingstepswiththeDDIM[84]
 sampler. : trainedinKL-regularizedlatentspace.Additionalre
sultscanbefoundinthesupplementary.
 Text-ConditionalImageSynthesis
 Method FID IS Nparams
 CogView [17] 27.10 18.20 4B self-ranking,rejectionrate0.017
 LAFITE [109] 26.94 26.02 75M
 GLIDE [59] 12.24- 6B 277DDIMsteps,c.f.g.[32]s=3
 Make-A-Scene [26] 11.84- 4B c.f.gforARmodels[98]s=5
 LDM-KL-8 23.31 20.03 0.33 1.45B 250DDIMsteps
 LDM-KL-8-G 12.63 30.29 0.42 1.45B 250DDIMsteps,c.f.g.[32]s=15
 Table2. Evaluationof text-conditional imagesynthesisonthe
 256 256-sizedMS-COCO[51]dataset: with250DDIM[84]
 stepsourmodel isonparwiththemostrecentdiffusion[59]and
 autoregressive [26]methodsdespiteusingsignificantly lesspa
rameters. / :Numbersfrom[109]/[26]
 Moreover, LDMs consistently improveuponGAN-based
 methods inPrecisionandRecall, thusconfirmingthead
vantagesof theirmode-coveringlikelihood-basedtraining
 objectiveover adversarial approaches. InFig. 4wealso
 showqualitativeresultsoneachdataset.
 6
Figure 8. Layout-to-image synthesis with an LDM on COCO [4],
 see Sec. 4.3.1. Quantitative evaluation in the supplement D.3.
 4.3. Conditional Latent Diffusion
 4.3.1 Transformer Encoders for LDMs
 By introducing cross-attention based conditioning into
 LDMs we open them up for various conditioning modali
ties previously unexplored for diffusion models. For text
to-image image modeling, we train a 1.45B parameter
 KL-regularized LDM conditioned on language prompts on
 LAION-400M [78]. We employ the BERT-tokenizer [14]
 and implement 
as a transformer [97] to infer a latent
 code which is mapped into the UNet via (multi-head) cross
attention (Sec. 3.3). This combination of domain specific
 experts for learning a language representation and visual
 synthesis results in a powerful model, which generalizes
 well to complex, user-defined text prompts, cf. Fig. 8 and 5.
 For quantitative analysis, we follow prior work and evaluate
 text-to-image generation on the MS-COCO [51] validation
 set, where our model improves upon powerful AR [17,66]
 and GAN-based [109] methods, cf. Tab. 2. We note that ap
plying classifier-free diffusion guidance [32] greatly boosts
 sample quality, such that the guided LDM-KL-8-G is on par
 with the recent state-of-the-art AR [26] and diffusion mod
els [59] for text-to-image synthesis, while substantially re
ducing parameter count. To further analyze the flexibility of
 the cross-attention based conditioning mechanism we also
 train models to synthesize images based on semantic lay
outs on OpenImages [49], and finetune on COCO [4], see
 Fig. 8. See Sec. D.3 for the quantitative evaluation and im
plementation details.
 Lastly, following prior work [3, 15, 21, 23], we evalu
ate our best-performing class-conditional ImageNet mod
els with f 
48 from Sec. 4.1 in Tab. 3, Fig. 4 and
 Sec. D.4. Here we outperform the state of the art diffu
sion model ADM [15] while significantly reducing compu
tational requirements and parameter count, cf. Tab 18.
 4.3.2 Convolutional Sampling Beyond 2562
 By concatenating spatially aligned conditioning informa
tion to the input of , LDMs can serve as efficient general
Method
 FID
 IS
 Precision
 Recall
 Nparams
 BigGan-deep [3]
 ADM[15]
 ADM-G[15]
 6.95
 10.94
 4.59
 203.6 2.6
 100.98
 186.7
 0.87
 0.69
 0.82
 0.28
 0.63
 0.52
 340M
 554M
 608M
250 DDIMsteps
 250 DDIMsteps
 LDM-4 (ours)
 LDM-4-G (ours)
 10.56
 3.60
 103.49 1.24
 247.67 5.59
 0.71
 0.87
 0.62
 0.48
 400M
 400M
 250 DDIMsteps
 250 steps, c.f.g [32], s = 15
 Table 3. Comparison of a class-conditional ImageNet LDM with
 recent state-of-the-art methods for class-conditional image gener
ation on ImageNet [12]. A more detailed comparison with addi
tional baselines can be found in D.4, Tab. 10 and F. c.f.g. denotes
 classifier-free guidance with a scale s as proposed in [32].
 purpose image-to-image translation models. We use this
 to train models for semantic synthesis, super-resolution
 (Sec. 4.4) and inpainting (Sec. 4.5). For semantic synthe
sis, we use images of landscapes paired with semantic maps
 [23, 61] and concatenate downsampled versions of the se
mantic maps with the latent image representation of a f = 4
 model (VQ-reg., see Tab. 8). We train on an input resolution
 of 2562 (crops from 3842) but find that our model general
izes to larger resolutions and can generate images up to the
 megapixel regime when evaluated in a convolutional man
ner (see Fig. 9). We exploit this behavior to also apply the
 super-resolution models in Sec. 4.4 and the inpainting mod
els in Sec. 4.5 to generate large images between 5122 and
 10242. For this application, the signal-to-noise ratio (in
duced by the scale of the latent space) significantly affects
 the results. In Sec. D.1 we illustrate this when learning an
 LDMon(i) the latent space as provided by a f = 4 model
 (KL-reg., see Tab. 8), and (ii) a rescaled version, scaled by
 the component-wise standard deviation.
 The latter, in combination with classifier-free guid
ance [32], also enables the direct synthesis of > 2562 im
ages for the text-conditional LDM-KL-8-G as in Fig. 13.
 Figure 9. A LDM trained on 2562 resolution can generalize to
 larger resolution (here: 512 1024) for spatially conditioned tasks
 such as semantic synthesis of landscape images. See Sec. 4.3.2.
 4.4. Super-Resolution with Latent Diffusion
 LDMs can be efficiently trained for super-resolution by
 diretly conditioning on low-resolution images via concate
nation (cf. Sec. 3.3). In a first experiment, we follow SR3
 7
bicubic LDM-SR SR3
 Figure10. ImageNet64 256super-resolutiononImageNet-Val.
 LDM-SRhasadvantagesat renderingrealistic texturesbutSR3
 cansynthesizemorecoherentfinestructures. Seeappendixfor
 additionalsamplesandcropouts.SR3resultsfrom[72].
 [72]andfixtheimagedegradationtoabicubicinterpola
tionwith4-downsamplingandtrainonImageNetfollow
ingSR3’sdataprocessingpipeline.Weusethef=4au
toencodingmodelpretrainedonOpenImages(VQ-reg.,cf.
 Tab.8)andconcatenatethelow-resolutionconditioningy
 andtheinputstotheUNet,i.e. istheidentity.Ourquali
tativeandquantitativeresults(seeFig.10andTab.5)show
 competitiveperformanceandLDM-SRoutperformsSR3
 inFIDwhileSR3hasabetterIS.Asimpleimageregres
sionmodel achieves thehighestPSNRandSSIMscores;
 however thesemetricsdonotalignwellwithhumanper
ception[106]andfavorblurrinessoverimperfectlyaligned
 high frequencydetails [72]. Further,weconduct auser
 studycomparingthepixel-baselinewithLDM-SR.Wefol
lowSR3[72]wherehumansubjectswereshownalow-res
 imageinbetweentwohigh-resimagesandaskedforpref
erence. TheresultsinTab.4affirmthegoodperformance
 ofLDM-SR.PSNRandSSIMcanbepushedbyusinga
 post-hocguidingmechanism[15]andwe implement this
 image-basedguiderviaaperceptual loss, seeSec.D.6.
 SRonImageNet InpaintingonPlaces
 UserStudy Pixel-DM(f1) LDM-4 LAMA[88] LDM-4
 Task1:PreferencevsGT 16.0% 30.4% 13.6% 21.0%
 Task2:PreferenceScore 29.4% 70.6% 31.9% 68.1%
 Table4.Task1:Subjectswereshowngroundtruthandgenerated
 imageandaskedforpreference. Task2: Subjectshadtodecide
 betweentwogeneratedimages.MoredetailsinE.3.6
 Sincethebicubicdegradationprocessdoesnotgeneralize
 welltoimageswhichdonotfollowthispre-processing,we
 alsotrainagenericmodel, LDM-BSR,byusingmoredi
versedegradation.TheresultsareshowninSec.D.6.1.
 Method FID IS PSNR SSIM Nparams [ samples
 s ]( )
 ImageRegression[72] 15.2 121.1 27.9 0.801 625M N/A
 SR3[72] 5.2 180.1 26.4 0.762 625M N/A
 LDM-4(ours,100steps) 2.8 /4.8 166.3 24.4 3.8 0.69 0.14 169M 4.62
 emphLDM-4(ours,big,100steps) 2.4 /4.3 174.9 24.7 4.1 0.71 0.15 552M 4.5
 LDM-4(ours,50steps,guiding) 4.4 /6.4 153.7 25.8 3.7 0.74 0.12 184M 0.38
 Table5. 4upscalingresultsonImageNet-Val. (2562); :FID
 featurescomputedonvalidationsplit, : FIDfeaturescomputed
 ontrainsplit; :AssessedonaNVIDIAA100
 trainthroughput samplingthroughput train+val FID@2k
 Model(reg.-type) samples/sec. @256 @512 hours/epoch epoch6
 LDM-1(nofirststage) 0.11 0.26 0.07 20.66 24.74
 LDM-4(KL,w/attn) 0.32 0.97 0.34 7.66 15.21
 LDM-4(VQ,w/attn) 0.33 0.97 0.34 7.04 14.99
 LDM-4(VQ,w/oattn) 0.35 0.99 0.36 6.66 15.95
 Table6.Assessinginpaintingefficiency. :DeviationsfromFig.7
 duetovaryingGPUsettings/batchsizescf.thesupplement.
 4.5.InpaintingwithLatentDiffusion
 Inpaintingisthetaskoffillingmaskedregionsofanim
agewithnewcontenteitherbecausepartsoftheimageare
 arecorruptedor toreplaceexistingbutundesiredcontent
 withintheimage.Weevaluatehowourgeneralapproach
 forconditionalimagegenerationcomparestomorespecial
ized, state-of-the-artapproachesfor thistask. Ourevalua
tionfollowstheprotocolofLaMa[88],arecent inpainting
 model that introducesaspecializedarchitecturerelyingon
 FastFourierConvolutions[8].Theexacttraining&evalua
tionprotocolonPlaces[108]isdescribedinSec.E.2.2.
 Wefirstanalyzetheeffectofdifferentdesignchoicesfor
 thefirststage. Inparticular,wecomparetheinpaintingef
ficiencyofLDM-1(i.e.apixel-basedconditionalDM)with
 LDM-4,forbothKLandVQregularizations,aswellasVQ
LDM-4withoutanyattentioninthefirststage(seeTab.8),
 wherethelatterreducesGPUmemoryfordecodingathigh
 resolutions.Forcomparability,wefixthenumberofparam
etersforallmodels.Tab.6reportsthetrainingandsampling
 throughputat resolution2562 and5122, thetotal training
 timeinhoursperepochandtheFIDscoreonthevalidation
 splitaftersixepochs.Overall,weobserveaspeed-upofat
 least27 betweenpixel-andlatent-baseddiffusionmodels
 whileimprovingFIDscoresbyafactorofatleast16 .
 The comparisonwith other inpainting approaches in
 Tab. 7shows that ourmodelwithattention improves the
 overallimagequalityasmeasuredbyFIDoverthatof[88].
 LPIPSbetweentheunmaskedimagesandour samples is
 slightlyhigher thanthatof [88].Weattributethis to[88]
 onlyproducingasingleresultwhichtendstorecovermore
 ofanaverage imagecomparedtothediverseresultspro
ducedbyourLDMcf.Fig.21.Additionallyinauserstudy
 (Tab.4)humansubjectsfavorourresultsoverthoseof[88].
 Basedontheseinitialresults,wealsotrainedalargerdif
fusionmodel(biginTab.7)inthelatentspaceoftheVQ
regularizedfirst stagewithout attention. Following[15],
 theUNetof thisdiffusionmodelusesattentionlayerson
 threelevelsofitsfeaturehierarchy,theBigGAN[3]residual
 blockforup-anddownsamplingandhas387Mparameters
 8
input result
 Figure11. Qualitativeresultsonobjectremovalwithourbig,w/
 ftinpaintingmodel.Formoreresults,seeFig.22.
 insteadof215M.After training,wenoticedadiscrepancy
 inthequalityofsamplesproducedatresolutions2562and
 5122,whichwehypothesizetobecausedbytheadditional
 attentionmodules.However,fine-tuningthemodelforhalf
 anepochat resolution5122allows themodel toadjust to
 thenewfeaturestatisticsandsetsanewstateoftheartFID
 onimageinpainting(big,w/oattn,w/ftinTab.7,Fig.11.).
 5.Limitations&SocietalImpact
 Limitations WhileLDMssignificantlyreducecomputa
tional requirements compared topixel-basedapproaches,
 their sequential samplingprocess is still slower than that
 ofGANs. Moreover, theuseofLDMscanbequestion
ablewhenhighprecisionisrequired: althoughthelossof
 imagequalityisverysmallinourf=4autoencodingmod
els(seeFig.1), theirreconstructioncapabilitycanbecome
 abottleneckfor tasksthat requirefine-grainedaccuracyin
 pixel space. Weassume that our superresolutionmodels
 (Sec.4.4)arealreadysomewhatlimitedinthisrespect.
 Societal Impact Generativemodels formedia like im
ageryareadouble-edgedsword: On theonehand, they
 40-50%masked Allsamples
 Method FID LPIPS FID LPIPS
 LDM-4(ours,big,w/ft) 9.39 0.246 0.042 1.50 0.137 0.080
 LDM-4(ours,big,w/oft) 12.89 0.257 0.047 2.40 0.142 0.085
 LDM-4(ours,w/attn) 11.87 0.257 0.042 2.15 0.144 0.084
 LDM-4(ours,w/oattn) 12.60 0.259 0.041 2.37 0.145 0.084
 LaMa[88] 12.31 0.243 0.038 2.23 0.134 0.080
 LaMa[88] 12.0 0.24 2.21 0.14
 CoModGAN[107] 10.4 0.26 1.82 0.15
 RegionWise[52] 21.3 0.27 4.75 0.15
 DeepFillv2[104] 22.1 0.28 5.20 0.16
 EdgeConnect[58] 30.5 0.28 8.37 0.16
 Table7. Comparisonof inpaintingperformanceon30kcropsof
 size512 512fromtestimagesofPlaces[108].Thecolumn40
50%reportsmetricscomputedoverhardexampleswhere40-50%
 oftheimageregionhavetobeinpainted. recomputedonourtest
 set,sincetheoriginaltestsetusedin[88]wasnotavailable.
 enablevariouscreativeapplications, andinparticularap
proaches likeours that reducethecostof trainingandin
ferencehavethepotential tofacilitateaccess tothis tech
nologyanddemocratizeitsexploration.Ontheotherhand,
 italsomeansthat itbecomeseasier tocreateanddissemi
natemanipulateddataorspreadmisinformationandspam.
 Inparticular, thedeliberatemanipulationofimages(“deep
 fakes”)isacommonprobleminthiscontext,andwomenin
 particulararedisproportionatelyaffectedbyit[13,24].
 Generativemodels canalso reveal their trainingdata
 [5,90],which isofgreat concernwhen thedatacontain
 sensitiveorpersonal informationandwerecollectedwith
outexplicitconsent.However,theextenttowhichthisalso
 appliestoDMsofimagesisnotyetfullyunderstood.
 Finally,deeplearningmodulestendtoreproduceorex
acerbatebiasesthatarealreadypresent inthedata[22,38,
 91].Whilediffusionmodelsachievebettercoverageofthe
 datadistributionthane.g.GAN-basedapproaches, theex
tent towhichourtwo-stageapproachthatcombinesadver
sarial traininganda likelihood-basedobjectivemisrepre
sentsthedataremainsanimportantresearchquestion.
 For amoregeneral, detaileddiscussionof theethical
 considerationsofdeepgenerativemodels,seee.g.[13].
 6.Conclusion
 Wehavepresentedlatentdiffusionmodels,asimpleand
 efficientwaytosignificantlyimproveboththetrainingand
 sampling efficiencyof denoisingdiffusionmodelswith
outdegradingtheirquality. Basedonthisandourcross
attentionconditioningmechanism, ourexperimentscould
 demonstratefavorableresultscomparedtostate-of-the-art
 methodsacrossawiderangeofconditionalimagesynthesis
 taskswithouttask-specificarchitectures.
 ThisworkhasbeensupportedbytheGermanFederalMinistryfor
 EconomicAffairsandEnergywithintheproject ’KI-Absicherung-Safe
 AIforautomateddriving’andbytheGermanResearchFoundation(DFG)
 project421703927.
 9
References
 [1] Eirikur Agustsson and Radu Timofte. NTIRE 2017 chal
lenge on single image super-resolution: Dataset and study.
 In 2017 IEEE Conference on Computer Vision and Pattern
 Recognition Workshops, CVPR Workshops 2017, Honolulu,
 HI, USA, July 21-26, 2017, pages 1122–1131. IEEE Com
puter Society, 2017. 1
 [2] Martin Arjovsky, Soumith Chintala, and L´ eon Bottou.
 Wasserstein gan, 2017. 3
 [3] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large
 scale GAN training for high fidelity natural image synthe
sis. In Int. Conf. Learn. Represent., 2019. 1, 2, 7, 8, 22,
 28
 [4] Holger Caesar, Jasper R. R. Uijlings, and Vittorio Ferrari.
 Coco-stuff: Thing and stuff classes in context. In 2018
 IEEE Conference on Computer Vision and Pattern Recog
nition, CVPR 2018, Salt Lake City, UT, USA, June 18
22, 2018, pages 1209–1218. Computer Vision Foundation /
 IEEE Computer Society, 2018. 7, 20, 22
 [5] Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew
 Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam
 Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al.
 Extracting training data from large language models. In
 30th USENIX Security Symposium (USENIX Security 21),
 pages 2633–2650, 2021. 9
 [6] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Hee
woo Jun, David Luan, and Ilya Sutskever. Generative pre
training from pixels. In ICML, volume 119 of Proceedings
 of Machine Learning Research, pages 1691–1703. PMLR,
 2020. 3
 [7] Nanxin Chen, Yu Zhang, Heiga Zen, Ron J. Weiss, Mo
hammad Norouzi, and William Chan. Wavegrad: Estimat
ing gradients for waveform generation. In ICLR. OpenRe
view.net, 2021. 1
 [8] Lu Chi, Borui Jiang, and Yadong Mu. Fast fourier convolu
tion. In NeurIPS, 2020. 8
 [9] Rewon Child. Very deep vaes generalize autoregressive
 models and can outperform them on images. CoRR,
 abs/2011.10650, 2020. 3
 [10] RewonChild, ScottGray, AlecRadford, andIlya Sutskever.
 Generating long sequences with sparse transformers.
 CoRR, abs/1904.10509, 2019. 3
 [11] BinDaiandDavidP.Wipf. DiagnosingandenhancingVAE
 models. In ICLR (Poster). OpenReview.net, 2019. 2, 3
 [12] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
 and Fei-Fei Li. Imagenet: A large-scale hierarchical im
age database. In CVPR, pages 248–255. IEEE Computer
 Society, 2009. 1, 5, 7, 22
 [13] Emily Denton. Ethical considerations of generative ai. AI
 for Content Creation Workshop, CVPR, 2021. 9
 [14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
 Kristina Toutanova. BERT: pre-training of deep bidirec
tional transformers for language understanding. CoRR,
 abs/1810.04805, 2018. 7
 [15] Prafulla Dhariwal and Alex Nichol. Diffusion models beat
 gans on image synthesis. CoRR, abs/2105.05233, 2021. 1,
 2, 3, 4, 6, 7, 8, 18, 22, 25, 26, 28
 [16] Sander Dieleman. Musings on typicality, 2020. 1, 3
 [17] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng,
 Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao,
 Hongxia Yang, and Jie Tang. Cogview: Mastering text-to
image generation via transformers. CoRR, abs/2105.13290,
 2021. 6, 7
 [18] Laurent Dinh, David Krueger, and Yoshua Bengio. Nice:
 Non-linear independent components estimation, 2015. 3
 [19] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Ben
gio. Density estimation using real NVP. In 5th Inter
national Conference on Learning Representations, ICLR
 2017, Toulon, France, April 24-26, 2017, Conference Track
 Proceedings. OpenReview.net, 2017. 1, 3
 [20] Alexey Dosovitskiy and Thomas Brox. Generating images
 with perceptual similarity metrics based on deep networks.
 In Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg,
 Isabelle Guyon, and Roman Garnett, editors, Adv. Neural
 Inform. Process. Syst., pages 658–666, 2016. 3
 [21] Patrick Esser, Robin Rombach, Andreas Blattmann, and
 Bj¨ orn Ommer. Imagebart: Bidirectional context with multi
nomial diffusion for autoregressive image synthesis. CoRR,
 abs/2108.08827, 2021. 6, 7, 22
 [22] Patrick Esser, Robin Rombach, and Bj¨ orn Ommer. A
 note on data biases in generative models. arXiv preprint
 arXiv:2012.02516, 2020. 9
 [23] Patrick Esser, Robin Rombach, and Bj¨ orn Ommer. Taming
 transformers for high-resolution image synthesis. CoRR,
 abs/2012.09841, 2020. 2, 3, 4, 6, 7, 21, 22, 29, 34, 36
 [24] Mary Anne Franks and Ari Ezra Waldman. Sex, lies, and
 videotape: Deep fakes and free speech delusions. Md. L.
 Rev., 78:892, 2018. 9
 [25] Kevin Frans, Lisa B. Soros, and Olaf Witkowski. Clipdraw:
 Exploring text-to-drawing synthesis through language
image encoders. ArXiv, abs/2106.14843, 2021. 3
 [26] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin,
 Devi Parikh, and Yaniv Taigman. Make-a-scene: Scene
based text-to-image generation with human priors. CoRR,
 abs/2203.13131, 2022. 6, 7, 16
 [27] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
 Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville,
 and Yoshua Bengio. Generative adversarial networks.
 CoRR, 2014. 1, 2
 [28] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent
 Dumoulin, and Aaron Courville. Improved training of
 wasserstein gans, 2017. 3
 [29] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
 Bernhard Nessler, and Sepp Hochreiter. Gans trained by
 a two time-scale update rule converge to a local nash equi
librium. In Adv. Neural Inform. Process. Syst., pages 6626
6637, 2017. 1, 5, 26
 [30] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif
fusion probabilistic models. In NeurIPS, 2020. 1, 2, 3, 4,
 6, 17
 [31] Jonathan Ho, Chitwan Saharia, William Chan, David J.
 Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded
 diffusion models for high fidelity image generation. CoRR,
 abs/2106.15282, 2021. 1, 3, 22
 10
[32] Jonathan Ho and Tim Salimans. Classifier-free diffusion
 guidance. In NeurIPS 2021 Workshop on Deep Generative
 Models and Downstream Applications, 2021. 6, 7, 16, 22,
 28, 37, 38
 [33] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A.
 Efros. Image-to-image translation with conditional adver
sarial networks. In CVPR, pages 5967–5976. IEEE Com
puter Society, 2017. 3, 4
 [34] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A.
 Efros. Image-to-image translation with conditional adver
sarial networks. 2017 IEEE Conference on Computer Vi
sion and Pattern Recognition (CVPR), pages 5967–5976,
 2017. 4
 [35] Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste
 Alayrac, Carl Doersch, Catalin Ionescu, David Ding,
 Skanda Koppula, Daniel Zoran, Andrew Brock, Evan
 Shelhamer, Olivier J. H´ enaff, Matthew M. Botvinick,
 Andrew Zisserman, Oriol Vinyals, and Jo˜ ao Carreira.
 Perceiver IO: A general architecture for structured inputs
 &outputs. CoRR, abs/2107.14795, 2021. 4
 [36] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals,
 Andrew Zisserman, and Jo˜ao Carreira. Perceiver: General
 perception with iterative attention. In Marina Meila and
 Tong Zhang, editors, Proceedings of the 38th International
 Conference on Machine Learning, ICML 2021, 18-24 July
 2021, Virtual Event, volume 139 of Proceedings of Machine
 Learning Research, pages 4651–4664. PMLR, 2021. 4, 5
 [37] Manuel Jahn, Robin Rombach, and Bj¨orn Ommer. High
resolution complex scene synthesis with transformers.
 CoRR, abs/2105.06458, 2021. 20, 22, 27
 [38] Niharika Jain, Alberto Olmo, Sailik Sengupta, Lydia
 Manikonda, and Subbarao Kambhampati. Imperfect ima
ganation: Implications of gans exacerbating biases on fa
cial data augmentation and snapchat selfie lenses. arXiv
 preprint arXiv:2001.09528, 2020. 9
 [39] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehti
nen. Progressive growing of gans for improved quality, sta
bility, and variation. CoRR, abs/1710.10196, 2017. 5, 6
 [40] Tero Karras, Samuli Laine, and Timo Aila. A style-based
 generator architecture for generative adversarial networks.
 In IEEE Conf. Comput. Vis. Pattern Recog., pages 4401
4410, 2019. 1
 [41] T. Karras, S. Laine, and T. Aila. A style-based gener
ator architecture for generative adversarial networks. In
 2019 IEEE/CVF Conference on Computer Vision and Pat
tern Recognition (CVPR), 2019. 5, 6
 [42] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,
 Jaakko Lehtinen, and Timo Aila. Analyzing and improv
ing the image quality of stylegan. CoRR, abs/1912.04958,
 2019. 2, 6, 28
 [43] Dongjun Kim, Seungjae Shin, Kyungwoo Song, Wanmo
 Kang, and Il-Chul Moon. Score matching model for un
bounded data score. CoRR, abs/2106.05527, 2021. 6
 [44] Durk P Kingma and Prafulla Dhariwal. Glow: Generative
 f
 lowwithinvertible 1x1 convolutions. In S. Bengio, H. Wal
lach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R.
 Garnett, editors, Advances in Neural Information Process
ing Systems, 2018. 3
 [45] Diederik P. Kingma, Tim Salimans, Ben Poole, and
 Jonathan Ho. Variational diffusion models. CoRR,
 abs/2107.00630, 2021. 1, 3, 16
 [46] DiederikP. KingmaandMaxWelling. Auto-EncodingVari
ational Bayes. In 2nd International Conference on Learn
ing Representations, ICLR, 2014. 1, 3, 4, 29
 [47] Zhifeng Kong and Wei Ping. On fast sampling of diffusion
 probabilistic models. CoRR, abs/2106.00132, 2021. 3
 [48] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and
 Bryan Catanzaro. Diffwave: A versatile diffusion model
 for audio synthesis. In ICLR. OpenReview.net, 2021. 1
 [49] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper R. R.
 Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali,
 Stefan Popov, Matteo Malloci, Tom Duerig, and Vittorio
 Ferrari. The open images dataset V4: unified image classi
f
 ication, object detection, and visual relationship detection
 at scale. CoRR, abs/1811.00982, 2018. 7, 20, 22
 [50] Tuomas Kynk¨ a¨ anniemi, Tero Karras, Samuli Laine, Jaakko
 Lehtinen, and Timo Aila. Improved precision and re
call metric for assessing generative models.
 CoRR,
 abs/1904.06991, 2019. 5, 26
 [51] Tsung-Yi Lin, Michael Maire, Serge J. Belongie,
 Lubomir D. Bourdev, Ross B. Girshick, James Hays, Pietro
 Perona, Deva Ramanan, Piotr Doll´ ar, and C. Lawrence Zit
nick. Microsoft COCO: common objects in context. CoRR,
 abs/1405.0312, 2014. 6, 7, 27
 [52] Yuqing Ma, Xianglong Liu, Shihao Bai, Le-Yi Wang, Ais
han Liu, Dacheng Tao, and Edwin Hancock. Region-wise
 generative adversarial imageinpainting for large missing ar
eas. ArXiv, abs/1909.12507, 2019. 9
 [53] Chenlin Meng, Yang Song, Jiaming Song, Jiajun Wu, Jun
Yan Zhu, and Stefano Ermon. Sdedit: Image synthesis
 and editing with stochastic differential equations. CoRR,
 abs/2108.01073, 2021. 1
 [54] Lars M.Mescheder. Ontheconvergence properties of GAN
 training. CoRR, abs/1801.04406, 2018. 3
 [55] Luke Metz, Ben Poole, David Pfau, and Jascha Sohl
Dickstein. Unrolled generative adversarial networks. In
 5th International Conference on Learning Representations,
 ICLR 2017, Toulon, France, April 24-26, 2017, Conference
 Track Proceedings. OpenReview.net, 2017. 3
 [56] Mehdi Mirza and Simon Osindero. Conditional generative
 adversarial nets. CoRR, abs/1411.1784, 2014. 4
 [57] Gautam Mittal, Jesse H. Engel, Curtis Hawthorne, and Ian
 Simon. Symbolic music generation with diffusion models.
 CoRR, abs/2103.16091, 2021. 1
 [58] Kamyar Nazeri, Eric Ng, Tony Joseph, Faisal Z. Qureshi,
 and Mehran Ebrahimi. Edgeconnect: Generative im
age inpainting with adversarial edge learning. ArXiv,
 abs/1901.00212, 2019. 9
 [59] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav
 Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and
 Mark Chen. GLIDE: towards photorealistic image genera
tion and editing with text-guided diffusion models. CoRR,
 abs/2112.10741, 2021. 6, 7, 16
 [60] Anton Obukhov, Maximilian Seitzer, Po-Wei Wu, Se
men Zhydenko, Jonathan Kyl, and Elvis Yu-Jing Lin.
 11
High-fidelity performance metrics for generative models
 in pytorch, 2020. Version: 0.3.0, DOI: 10.5281/zen
odo.4957738. 26, 27
 [61] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun
Yan Zhu. Semantic image synthesis with spatially-adaptive
 normalization. In Proceedings of the IEEE Conference on
 Computer Vision and Pattern Recognition, 2019. 4, 7
 [62] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun
Yan Zhu. Semantic image synthesis with spatially-adaptive
 normalization. In Proceedings of the IEEE/CVF Confer
ence on Computer Vision and Pattern Recognition (CVPR),
 June 2019. 22
 [63] Gaurav Parmar, Dacheng Li, Kwonjoon Lee, and Zhuowen
 Tu. Dual contradistinctive generative autoencoder. In IEEE
 Conference on Computer Vision and Pattern Recognition,
 CVPR 2021, virtual, June 19-25, 2021, pages 823–832.
 Computer Vision Foundation / IEEE, 2021. 6
 [64] Gaurav Parmar, Richard Zhang, and Jun-Yan Zhu. On
 buggy resizing libraries and surprising subtleties in fid cal
culation. arXiv preprint arXiv:2104.11222, 2021. 26
 [65] David A. Patterson, Joseph Gonzalez, Quoc V. Le, Chen
 Liang, Lluis-Miquel Munguia, Daniel Rothchild, David R.
 So, Maud Texier, and Jeff Dean. Carbon emissions and
 large neural network training. CoRR, abs/2104.10350,
 2021. 2
 [66] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott
 Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya
 Sutskever. Zero-shot text-to-image generation. CoRR,
 abs/2102.12092, 2021. 1, 2, 3, 4, 7, 21, 27
 [67] Ali Razavi, A¨aron van den Oord, and Oriol Vinyals. Gen
erating diverse high-fidelity images with VQ-VAE-2. In
 NeurIPS, pages 14837–14847, 2019. 1, 2, 3, 22
 [68] Scott E. Reed, Zeynep Akata, Xinchen Yan, Lajanugen Lo
geswaran, Bernt Schiele, and Honglak Lee. Generative ad
versarial text to image synthesis. In ICML, 2016. 4
 [69] Danilo Jimenez Rezende, Shakir Mohamed, and Daan
 Wierstra. Stochastic backpropagation and approximate in
ference in deep generative models. In Proceedings of the
 31st International Conference on International Conference
 on Machine Learning, ICML, 2014. 1, 4, 29
 [70] Robin Rombach, Patrick Esser, and Bj¨ orn Ommer.
 Network-to-network translation with conditional invertible
 neural networks. In NeurIPS, 2020. 3
 [71] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U
net: Convolutional networks for biomedical image segmen
tation. In MICCAI (3), volume 9351 of Lecture Notes in
 Computer Science, pages 234–241. Springer, 2015. 2, 3, 4
 [72] Chitwan Saharia, Jonathan Ho, William Chan, Tim Sal
imans, David J. Fleet, and Mohammad Norouzi. Im
age super-resolution via iterative refinement.
 CoRR,
 abs/2104.07636, 2021. 1, 4, 8, 16, 22, 23, 27
 [73] Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P.
 Kingma. Pixelcnn++: Improving the pixelcnn with dis
cretized logistic mixture likelihood and other modifications.
 CoRR, abs/1701.05517, 2017. 1, 3
 [74] Dave Salvator. NVIDIA Developer Blog. https:
 //developer.nvidia.com/blog/getting
immediate-speedups-with-a100-tf32, 2020.
 28
 [75] Robin San-Roman, Eliya Nachmani, and Lior Wolf.
 Noise estimation for generative diffusion models. CoRR,
 abs/2104.02600, 2021. 3
 [76] Axel Sauer, Kashyap Chitta, Jens M¨ uller, and An
dreas Geiger. Projected gans converge faster. CoRR,
 abs/2111.01007, 2021. 6
 [77] Edgar Sch¨ onfeld, Bernt Schiele, and Anna Khoreva. A u
net based discriminator for generative adversarial networks.
 In 2020 IEEE/CVF Conference on Computer Vision and
 Pattern Recognition, CVPR 2020, Seattle, WA, USA, June
 13-19, 2020, pages 8204–8213. Computer Vision Founda
tion / IEEE, 2020. 6
 [78] Christoph Schuhmann, Richard Vencu, Romain Beaumont,
 Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo
 Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion
400m: Open dataset of clip-filtered 400 million image-text
 pairs, 2021. 6, 7
 [79] Karen Simonyan and Andrew Zisserman. Very deep con
volutional networks for large-scale image recognition. In
 Yoshua Bengio and Yann LeCun, editors, Int. Conf. Learn.
 Represent., 2015. 29, 43, 44, 45
 [80] Abhishek Sinha, Jiaming Song, Chenlin Meng, and Stefano
 Ermon. D2C:diffusion-denoising models for few-shot con
ditional generation. CoRR, abs/2106.06819, 2021. 3
 [81] Charlie Snell. Alien Dreams: An Emerging Art Scene.
 https://ml.berkeley.edu/blog/posts/
 clip-art/, 2021. [Online; accessed November-2021].
 2
 [82] Jascha Sohl-Dickstein, Eric A. Weiss, Niru Mah
eswaranathan, and Surya Ganguli. Deep unsupervised
 learning using nonequilibrium thermodynamics. CoRR,
 abs/1503.03585, 2015. 1, 3, 4, 18
 [83] Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learn
ing structured output representation using deep conditional
 generative models. In C. Cortes, N. Lawrence, D. Lee,
 M. Sugiyama, and R. Garnett, editors, Advances in Neural
 Information Processing Systems, volume 28. Curran Asso
ciates, Inc., 2015. 4
 [84] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois
ing diffusion implicit models. In ICLR. OpenReview.net,
 2021. 3, 5, 6, 22
 [85] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma,
 Abhishek Kumar, Stefano Ermon, and Ben Poole. Score
based generative modeling through stochastic differential
 equations. CoRR, abs/2011.13456, 2020. 1, 3, 4, 18
 [86] Emma Strubell, Ananya Ganesh, and Andrew McCallum.
 Energy and policy considerations for modern deep learn
ing research. In The Thirty-Fourth AAAI Conference on
 Artificial Intelligence, AAAI 2020, The Thirty-Second In
novative Applications of Artificial Intelligence Conference,
 IAAI 2020, The Tenth AAAI Symposium on Educational
 Advances in Artificial Intelligence, EAAI 2020, New York,
 NY, USA, February 7-12, 2020, pages 13693–13696. AAAI
 Press, 2020. 2
 12
[87] Wei Sun and Tianfu Wu. Learning layout and style re
configurable gans for controllable image synthesis. CoRR,
 abs/2003.11571, 2020. 22, 27
 [88] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin,
 Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov,
 Naejin Kong, Harshith Goka, Kiwoong Park, and Victor S.
 Lempitsky. Resolution-robust large mask inpainting with
 fourier convolutions. ArXiv, abs/2109.07161, 2021. 8, 9,
 26, 32
 [89] Tristan Sylvain, Pengchuan Zhang, Yoshua Bengio, R. De
von Hjelm, and Shikhar Sharma. Object-centric image gen
eration from layouts. In Thirty-Fifth AAAI Conference on
 Artificial Intelligence, AAAI 2021, Thirty-Third Conference
 on Innovative Applications of Artificial Intelligence, IAAI
 2021, The Eleventh Symposium on Educational Advances
 in Artificial Intelligence, EAAI 2021, Virtual Event, Febru
ary 2-9, 2021, pages 2647–2655. AAAI Press, 2021. 20,
 22, 27
 [90] Patrick Tinsley, Adam Czajka, and Patrick Flynn. This face
 does not exist... but it might be yours! identity leakage in
 generative models. In Proceedings of the IEEE/CVF Win
ter Conference on Applications of Computer Vision, pages
 1320–1328, 2021. 9
 [91] Antonio Torralba and Alexei A Efros. Unbiased look at
 dataset bias. In CVPR 2011, pages 1521–1528. IEEE, 2011.
 9
 [92] Arash Vahdat and Jan Kautz. NVAE: A deep hierarchical
 variational autoencoder. In NeurIPS, 2020. 3
 [93] Arash Vahdat, Karsten Kreis, and Jan Kautz. Score
based generative modeling in latent space.
 CoRR,
 abs/2106.05931, 2021. 2, 3, 5, 6
 [94] Aaron van den Oord, Nal Kalchbrenner, Lasse Espeholt,
 koray kavukcuoglu, Oriol Vinyals, and Alex Graves. Con
ditional image generation with pixelcnn decoders. In Ad
vances in Neural Information Processing Systems, 2016. 3
 [95] A¨ aron van den Oord, Nal Kalchbrenner, and Koray
 Kavukcuoglu. Pixel recurrent neural networks. CoRR,
 abs/1601.06759, 2016. 3
 [96] A¨ aron van den Oord, Oriol Vinyals, and Koray
 Kavukcuoglu. Neural discrete representation learning. In
 NIPS, pages 6306–6315, 2017. 2, 4, 29
 [97] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
 Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,
 and Illia Polosukhin. Attention is all you need. In NIPS,
 pages 5998–6008, 2017. 3, 4, 5, 7
 [98] Rivers Have Wings.
 Tweet on Classifier-free
 guidance for autoregressive models.
 https :
 //twitter.com/RiversHaveWings/status/
 1478093658716966912, 2022. 6
 [99] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chau
mond, Clement Delangue, Anthony Moi, Pierric Cistac,
 Tim Rault, R´ emi Louf, Morgan Funtowicz, and Jamie
 Brew. Huggingface’s transformers: State-of-the-art natural
 language processing. CoRR, abs/1910.03771, 2019. 26
 [100] Zhisheng Xiao, Karsten Kreis, Jan Kautz, and Arash Vah
dat. VAEBM: A symbiosis between variational autoen
coders and energy-based models. In 9th International Con
ference on Learning Representations, ICLR 2021, Virtual
 Event, Austria, May 3-7, 2021. OpenReview.net, 2021. 6
 [101] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind
 Srinivas. Videogpt: Video generation using VQ-VAE and
 transformers. CoRR, abs/2104.10157, 2021. 3
 [102] Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianx
iong Xiao. LSUN: construction of a large-scale image
 dataset using deep learning with humans in the loop. CoRR,
 abs/1506.03365, 2015. 5
 [103] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, RuomingPang,
 James Qin, Alexander Ku, YuanzhongXu, JasonBaldridge,
 and Yonghui Wu. Vector-quantized image modeling with
 improved vqgan, 2021. 3, 4
 [104] Jiahui Yu, Zhe L. Lin, Jimei Yang, Xiaohui Shen, Xin Lu,
 and Thomas S. Huang. Free-form image inpainting with
 gated convolution. 2019 IEEE/CVF International Confer
ence on Computer Vision (ICCV), pages 4470–4479, 2019.
 9
 [105] K. Zhang, Jingyun Liang, Luc Van Gool, and Radu Timo
fte. Designing a practical degradation model for deep blind
 image super-resolution. ArXiv, abs/2103.14006, 2021. 23
 [106] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shecht
man, and Oliver Wang. The unreasonable effectiveness of
 deep features as a perceptual metric. In Proceedings of the
 IEEE Conference on Computer Vision and Pattern Recog
nition (CVPR), June 2018. 3, 8, 19
 [107] Shengyu Zhao, Jianwei Cui, Yilun Sheng, Yue Dong, Xiao
 Liang, Eric I-Chao Chang, and Yan Xu. Large scale image
 completion via co-modulated generative adversarial net
works. ArXiv, abs/2103.10428, 2021. 9
 [108] Bolei Zhou, ` Agata Lapedriza, Aditya Khosla, Aude Oliva,
 and Antonio Torralba. Places: A 10 million image database
 for scene recognition. IEEE Transactions on Pattern Anal
ysis and Machine Intelligence, 40:1452–1464, 2018. 8, 9,
 26
 [109] Yufan Zhou, Ruiyi Zhang, Changyou Chen, Chunyuan Li,
 Chris Tensmeyer, Tong Yu, Jiuxiang Gu, Jinhui Xu, and
 Tong Sun. LAFITE: towards language-free training for
 text-to-image generation. CoRR, abs/2111.13792, 2021. 6,
 7, 16
 13
Appendix
 Figure 12. Convolutional samples from the semantic landscapes model as in Sec. 4.3.2, finetuned on 5122 images.
 14
’A painting of the last supper by Picasso.’
 ’An oil painting of a latent space.’
 ’An epic painting of Gandalf the Black
 summoning thunder and lightning in the mountains.’
 ’A sunset over a mountain range, vector image.’
 Figure 13. Combining classifier free diffusion guidance with the convolutional sampling strategy from Sec. 4.3.2, our 1.45B parameter
 text-to-image model can be used for rendering images larger than the native 2562 resolution the model was trained on.
 15
A. Changelog
 Here we list changes between this version (https://arxiv.org/abs/2112.10752v2) of the paper and the
 previous version, i.e. https://arxiv.org/abs/2112.10752v1.
 • Weupdatedtheresults on text-to-image synthesis in Sec. 4.3 which were obtained by training a new, larger model (1.45B
 parameters). This also includes a new comparison to very recent competing methods on this task that were published on
 arXiv at the same time as ( [59,109]) or after ( [26]) the publication of our work.
 • We updated results on class-conditional synthesis on ImageNet in Sec. 4.1, Tab. 3 (see also Sec. D.4) obtained by
 retraining the model with a larger batch size. The corresponding qualitative results in Fig. 26 and Fig. 27 were also
 updated. Both the updated text-to-image and the class-conditional model now use classifier-free guidance [32] as a
 measure to increase visual fidelity.
 • Weconducted a user study (following the scheme suggested by Saharia et al [72]) which provides additional evaluation
 for our inpainting (Sec. 4.5) and superresolution models (Sec. 4.4).
 • Added Fig. 5 to the main paper, moved Fig. 18 to the appendix, added Fig. 13 to the appendix.
 B. Detailed Information on Denoising Diffusion Models
 Diffusion models can be specified in terms of a signal-to-noise ratio SNR(t) = 2
 t
 2
 t
 ( t)T
 t=1 which, starting from a data sample x0, define a forward diffusion process q as
 q(xtx0) = N(xt tx0 2
 tI)
 with the Markov structure for s < t:
 q(xtxs) = N(xt tsxs 2
 tsI)
 ts = t
 s
 2
 ts = 2
 t 
2
 ts
 2
 s
 consisting of sequences ( t)T
 t=1 and
 (4)
 (5)
 (6)
 (7)
 Denoising diffusion models are generative models p(x0) which revert this process with a similar Markov structure running
 backward in time, i.e. they are specified as
 T
 p(x0) =
 p(xT)
 z
 t=1
 p(xt 1xt)
 The evidence lower bound (ELBO) associated with this model then decomposes over the discrete time steps as
 T
 log p(x0) KL(q(xTx0)p(xT))+
 t=1
 Eq(xtx0)KL(q(xt 1xt x0)p(xt 1xt))
 (8)
 (9)
 The prior p(xT) is typically choosen as a standard normal distribution and the first term of the ELBO then depends only on
 the final signal-to-noise ratio SNR(T). To minimize the remaining terms, a common choice to parameterize p(xt 1xt) is to
 specify it in terms of the true posterior q(xt 1xt x0) but with the unknown x0 replaced by an estimate x(xt t) based on
 the current step xt. This gives [45]
 p(xt 1xt) := q(xt 1xt x(xt t))
 =N(xt 1 (xt t) 2
 tt 1
 where the mean can be expressed as
 (xt t) = tt 1 2
 t 1
 2
 t
 xt + t 1 2
 tt 1
 2
 t
 (10)
 2
 t 1
 2
 t
 I)
 x(xt t)
 (11)
 (12)
 16
In this case, the sum of the ELBO simplify to
 T
 t=1
 Following [30], we use the reparameterization
 T
 Eq(xtx0)KL(q(xt 1xt x0)p(xt 1) =
 t=1
 EN( 0I)
 1
 2 (SNR(t 1) SNR(t)) x0 x( tx0 + t t) 2 (13)
 (xt t) = (xt 
to express the reconstruction term as a denoising objective,
 x0 x( tx0+ t t) 2= 
tx(xt t)) t
 2
 t
 2
 t
 ( tx0 + t t) 2
 and the reweighting, which assigns each of the terms the same weight and results in Eq. (1).
 (14)
 (15)
 17
C. Image Guiding Mechanisms
 Samples 2562
 Guided Convolutional Samples 5122 Convolutional Samples 5122
 Figure 14. On landscapes, convolutional sampling with unconditional models can lead to homogeneous and incoherent global structures
 (see column 2). L2-guiding with a low resolution image can help to reestablish coherent global structures.
 An intriguing feature of diffusion models is that unconditional models can be conditioned at test-time [15, 82, 85]. In
 particular, [15] presented an algorithm to guide both unconditional and conditional models trained on the ImageNet dataset
 with a classifier logp (yxt), trained on each xt of the diffusion process. We directly build on this formulation and introduce
 post-hoc image-guiding:
 For an epsilon-parameterized model with fixed variance, the guiding algorithm as introduced in [15] reads:
 (zt t) +
 1 
2
 t 
zt 
log p (yzt) 
(16)
 This can be interpreted as an update correcting the “score” with a conditional distribution logp (yzt).
 So far, this scenario has only been applied to single-class classification models. We re-interpret the guiding distribution
 p (yT(D(z0(zt)))) as a general purpose image-to-image translation task given a target image y, where T can be any
 differentiable transformation adopted to the image-to-image translation task at hand, such as the identity, a downsampling
 operation or similar.
 18
As an example, we can assume a Gaussian guider with fixed variance 2 = 1, such that
 log p (yzt) = 1
 2 y T(D(z0(zt))) 2
 2
 becomes a L2 regression objective.
 (17)
 Fig. 14 demonstrates how this formulation can serve as an upsampling mechanism of an unconditional model trained on
 2562 images, where unconditional samples of size 2562 guide the convolutional synthesis of 5122 images and T is a 2
 bicubic downsampling. Following this motivation, we also experiment with a perceptual similarity guiding and replace the
 L2 objective with the LPIPS [106] metric, see Sec. 4.4.
 19
D. Additional Results
 D.1. Choosing the Signal-to-Noise Ratio for High-Resolution Synthesis
 KL-reg, w/o rescaling
 KL-reg, w/ rescaling
 VQ-reg, w/o rescaling
 Figure 15. Illustrating the effect of latent space rescaling on convolutional sampling, here for semantic image synthesis on landscapes. See
 Sec. 4.3.2 and Sec. D.1.
 Asdiscussed in Sec. 4.3.2, the signal-to-noise ratio induced by the variance of the latent space (i.e. Var(z) 2
 t) significantly
 affects the results for convolutional sampling. For example, when training a LDM directly in the latent space of a KL
regularized model (see Tab. 8), this ratio is very high, such that the model allocates a lot of semantic detail early on in the
 reverse denoising process. In contrast, when rescaling the latent space by the component-wise standard deviation of the
 latents as described in Sec. G, the SNR is descreased. We illustrate the effect on convolutional sampling for semantic image
 synthesis in Fig. 15. Note that the VQ-regularized space has a variance close to 1, such that it does not have to be rescaled.
 D.2. Full List of all First Stage Models
 Weprovide a complete list of various autoenconding models trained on the OpenImages dataset in Tab. 8.
 D.3. Layout-to-Image Synthesis
 Here we provide the quantitative evaluation and additional samples for our layout-to-image models from Sec. 4.3.1. We
 train a model on the COCO [4] and one on the OpenImages [49] dataset, which we subsequently additionally finetune on
 COCO. Tab 9 shows the result. Our COCO model reaches the performance of recent state-of-the art models in layout-to
image synthesis, when following their training and evaluation protocol [89]. When finetuning from the OpenImages model,
 we surpass these works. Our OpenImages model surpasses the results of Jahn et al [37] by a margin of nearly 11 in terms of
 FID. In Fig. 16 we show additional samples of the model finetuned on COCO.
 D.4. Class-Conditional Image Synthesis on ImageNet
 Tab. 10 contains the results for our class-conditional LDM measured in FID and Inception score (IS). LDM-8 requires
 significantly fewer parameters and compute requirements (see Tab. 18) to achieve very competitive performance. Similar
 to previous work, we can further boost the performance by training a classifier on each noise scale and guiding with it,
 20
f Z c R-FID R-IS PSNR PSIM SSIM
 16VQGAN[23] 16384 256 4.98– 19.9 34 1.83 042 0.51 018
 16VQGAN[23] 1024 256 7.94– 19.4 33 1.98 043 0.50 018
 8DALL-E[66] 8192- 32.01– 22.8 21 1.95 051 0.73 013
 32 16384 16 31.83 40.40 107 17.45 290 2.58 048 0.41 018
 16 16384 8 5.15 144.55 374 20.83 361 1.73 043 0.54 018
 8 16384 4 1.14 201.92 397 23.07 399 1.17 036 0.65 016
 8 256 4 1.49 194.20 387 22.35 381 1.26 037 0.62 016
 4 8192 3 0.58 224.78 535 27.43 426 0.53 021 0.82 010
 4 8192 3 1.06 221.94 458 25.21 417 0.72 026 0.76 012
 4 256 3 0.47 223.81 458 26.43 422 0.62 024 0.80 011
 2 2048 2 0.16 232.75 509 30.85 412 0.27 012 0.91 005
 2 64 2 0.40 226.62 483 29.13 346 0.38 013 0.90 005
 32 KL 64 2.04 189.53 368 22.27 393 1.41 040 0.61 017
 32 KL 16 7.3 132.75 271 20.38 356 1.88 045 0.53 018
 16 KL 16 0.87 210.31 397 24.08 422 1.07 036 0.68 015
 16 KL 8 2.63 178.68 408 21.94 392 1.49 042 0.59 017
 8 KL 4 0.90 209.90 492 24.19 419 1.02 035 0.69 015
 4 KL 3 0.27 227.57 489 27.53 454 0.55 024 0.82 011
 2 KL 2 0.086 232.66 516 32.47 419 0.20 009 0.93 004
 Table8. CompleteautoencoderzootrainedonOpenImages,evaluatedonImageNet-Val. denotesanattention-freeautoencoder.
 layout-to-imagesynthesisontheCOCOdataset
 Figure16. Moresamplesfromourbestmodelforlayout-to-imagesynthesis,LDM-4,whichwastrainedontheOpenImagesdatasetand
 finetunedontheCOCOdataset.Samplesgeneratedwith100DDIMstepsand =0.LayoutsarefromtheCOCOvalidationset.
 seeSec.C.Unlikethepixel-basedmethods, thisclassifieristrainedverycheaplyinlatentspace. Foradditionalqualitative
 results,seeFig.26andFig.27.
 21
COCO256 256 OpenImages256 256 OpenImages512 512
 Method FID FID FID
 LostGAN-V2[87] 42.55-
OC-GAN[89] 41.65-
SPADE[62] 41.11-
VQGAN+T[37] 56.58 45.33 48.11
 LDM-8(100steps,ours) 42.06-
LDM-4(200steps,ours) 40.91 32.02 35.80
 Table9. Quantitativecomparisonofourlayout-to-imagemodelsontheCOCO[4]andOpenImages[49]datasets. :Trainingfromscratch
 onCOCO; :FinetuningfromOpenImages.
 Method FID IS Precision Recall Nparams
 SR3[72] 11.30--- 625M
ImageBART[21] 21.19--- 3.5B
ImageBART[21] 7.44--- 3.5B 0.05acc. rate
 VQGAN+T[23] 17.04 70.6 1.8-- 1.3B
VQGAN+T[23] 5.88 304.8 3.6-- 1.3B 0.05acc. rate
 BigGan-deep[3] 6.95 203.6 2.6 0.87 0.28 340M
ADM[15] 10.94 100.98 0.69 0.63 554M 250DDIMsteps
 ADM-G[15] 4.59 186.7 0.82 0.52 608M 250DDIMsteps
 ADM-G,ADM-U[15] 3.85 221.72 0.84 0.53 n/a 2 250DDIMsteps
 CDM[31] 4.88 158.71 2.26-- n/a 2 100DDIMsteps
 LDM-8(ours) 17.41 72.92 2.6 0.65 0.62 395M 200DDIMsteps,2.9Mtrainsteps,batchsize64
 LDM-8-G(ours) 8.11 190.43 2.60 0.83 0.36 506M 200DDIMsteps,classifierscale10,2.9Mtrainsteps,batchsize64
 LDM-8(ours) 15.51 79.03 1.03 0.65 0.63 395M 200DDIMsteps,4.8Mtrainsteps,batchsize64
 LDM-8-G(ours) 7.76 209.52 4.24 0.84 0.35 506M 200DDIMsteps,classifierscale10,4.8Mtrainsteps,batchsize64
 LDM-4(ours) 10.56 103.49 1.24 0.71 0.62 400M 250DDIMsteps,178Ktrainsteps,batchsize1200
 LDM-4-G(ours) 3.95 178.22 2.43 0.81 0.55 400M 250DDIMsteps,unconditionalguidance[32]scale1.25,178Ktrainsteps,batchsize1200
 LDM-4-G(ours) 3.60 247.67 5.59 0.87 0.48 400M 250DDIMsteps,unconditionalguidance[32]scale1.5,178Ktrainsteps,batchsize1200
 Table10. Comparisonofaclass-conditionalImageNetLDMwithrecentstate-of-the-artmethodsforclass-conditionalimagegeneration
 ontheImageNet[12]dataset. :Classifierrejectionsamplingwiththegivenrejectionrateasproposedin[67].
 D.5.SampleQualityvs.V100Days(ContinuedfromSec.4.1)
 Figure17. Forcompletenesswealsoreport thetrainingprogressofclass-conditionalLDMsontheImageNetdatasetforafixednumber
 of35V100days.Resultsobtainedwith100DDIMsteps[84]and =0.FIDscomputedon5000samplesforefficiencyreasons.
 FortheassessmentofsamplequalityoverthetrainingprogressinSec.4.1,wereportedFIDandISscoresasafunction
 of trainsteps. Anotherpossibilityis toreport thesemetricsover theusedresources inV100days. Suchananalysis is
 additionallyprovidedinFig.17,showingqualitativelysimilarresults.
 22
Method
 FID 
IS 
PSNR
 SSIM 
Image Regression [72]
 SR3 [72]
 15.2
 5.2
 121.1
 180.1
 27.9
 26.4
 0.801
 0.762
 LDM-4 (ours, 100 steps)
 LDM-4 (ours, 50 steps, guiding)
 LDM-4 (ours, 100 steps, guiding)
 2.8 /4.8
 4.4 /6.4
 4.4 /6.4
 166.3
 153.7
 154.1
 24.4 3.8
 25.8 3.7
 25.7 3.7
 0.69 0.14
 0.74 0.12
 0.73 0.12
 LDM-4 (ours, 100 steps, +15 ep.)
 Pixel-DM (100 steps, +15 ep.)
 2.6 / 4.6
 5.1 / 7.1
 169.76 5.03
 163.06 4.67
 24.4 3.8
 24.1 3.3
 0.69 0.14
 0.59 0.12
 Table 11. 4 upscaling results on ImageNet-Val. (2562); : FID features computed on validation split, : FID features computed on train
 split. We also include a pixel-space baseline that receives the same amount of compute as LDM-4. The last two rows received 15 epochs
 of additional training compared to the former results.
 D.6. Super-Resolution
 For better comparability between LDMs and diffusion models in pixel space, we extend our analysis from Tab. 5 by
 comparing a diffusion model trained for the same number of steps and with a comparable number 1 of parameters to our
 LDM. The results of this comparison are shown in the last two rows of Tab. 11 and demonstrate that LDM achieves better
 performance while allowing for significantly faster sampling. A qualitative comparison is given in Fig. 20 which shows
 random samples from both LDM and the diffusion model in pixel space.
 D.6.1 LDM-BSR:General Purpose SR Model via Diverse Image Degradation
 bicubic
 LDM-SR
 LDM-BSR
 Figure 18. LDM-BSR generalizes to arbitrary inputs and can be used as a general-purpose upsampler, upscaling samples from a class
conditional LDM (image cf. Fig. 4) to 10242 resolution. In contrast, using a fixed degradation process (see Sec. 4.4) hinders generalization.
 Toevaluate generalization of our LDM-SR, we apply it both on synthetic LDM samples from a class-conditional ImageNet
 model (Sec. 4.1) and images crawled from the internet. Interestingly, we observe that LDM-SR, trained only with a bicubicly
 downsampled conditioning as in [72], does not generalize well to images which do not follow this pre-processing. Hence, to
 obtain a superresolution model for a wide range of real world images, which can contain complex superpositions of camera
 noise, compression artifacts, blurr and interpolations, we replace the bicubic downsampling operation in LDM-SR with the
 degration pipeline from [105]. The BSR-degradation process is a degradation pipline which applies JPEG compressions
 noise, camera sensor noise, different image interpolations for downsampling, Gaussian blur kernels and Gaussian noise in a
 random order to an image. We found that using the bsr-degredation process with the original parameters as in [105] leads to
 a very strong degradation process. Since a more moderate degradation process seemed apppropiate for our application, we
 adapted the parameters of the bsr-degradation (our adapted degradation process can be found in our code base at https:
 //github.com/CompVis/latent-diffusion). Fig. 18 illustrates the effectiveness of this approach by directly
 comparing LDM-SR with LDM-BSR. The latter produces images much sharper than the models confined to a fixed pre
processing, making it suitable for real-world applications. Further results of LDM-BSR are shown on LSUN-cows in Fig. 19.
 1It is not possible to exactly match both architectures since the diffusion model operates in the pixel space
 23
E.ImplementationDetailsandHyperparameters
 E.1.Hyperparameters
 WeprovideanoverviewofthehyperparametersofalltrainedLDMmodelsinTab.12,Tab.13,Tab.14andTab.15.
 CelebA-HQ256 256 FFHQ256 256 LSUN-Churches256 256 LSUN-Bedrooms256 256
 f 4 4 8 4
 z-shape 64 64 3 64 64 3-64 64 3
 Z 8192 8192- 8192
 Diffusionsteps 1000 1000 1000 1000
 NoiseSchedule linear linear linear linear
 Nparams 274M 274M 294M 274M
 Channels 224 224 192 224
 Depth 2 2 2 2
 ChannelMultiplier 1,2,3,4 1,2,3,4 1,2,2,4,4 1,2,3,4
 Attentionresolutions 32,16,8 32,16,8 32,16,8,4 32,16,8
 HeadChannels 32 32 24 32
 BatchSize 48 42 96 48
 Iterations 410k 635k 500k 1.9M
 LearningRate 9.6e-5 8.4e-5 5.e-5 9.6e-5
 Table12. HyperparametersfortheunconditionalLDMsproducingthenumbersshowninTab.1.AllmodelstrainedonasingleNVIDIA
 A100.
 LDM-1 LDM-2 LDM-4 LDM-8 LDM-16 LDM-32
 z-shape 256 256 3 128 128 2 64 64 3 32 32 4 16 16 8 88 8 32
 Z- 2048 8192 16384 16384 16384
 Diffusionsteps 1000 1000 1000 1000 1000 1000
 NoiseSchedule linear linear linear linear linear linear
 ModelSize 396M 391M 391M 395M 395M 395M
 Channels 192 192 192 256 256 256
 Depth 2 2 2 2 2 2
 ChannelMultiplier 1,1,2,2,4,4 1,2,2,4,4 1,2,3,5 1,2,4 1,2,4 1,2,4
 NumberofHeads 1 1 1 1 1 1
 BatchSize 7 9 40 64 112 112
 Iterations 2M 2M 2M 2M 2M 2M
 LearningRate 4.9e-5 6.3e-5 8e-5 6.4e-5 4.5e-5 4.5e-5
 Conditioning CA CA CA CA CA CA
 CA-resolutions 32,16,8 32,16,8 32,16,8 32,16,8 16,8,4 8,4,2
 EmbeddingDimension 512 512 512 512 512 512
 TransformersDepth 1 1 1 1 1 1
 Table13. HyperparametersfortheconditionalLDMstrainedontheImageNetdatasetfortheanalysisinSec.4.1.Allmodelstrainedona
 singleNVIDIAA100.
 E.2.ImplementationDetails
 E.2.1 Implementationsof forconditionalLDMs
 For theexperimentsontext-to-imageandlayout-to-image(Sec.4.3.1) synthesis,we implement theconditioner asan
 unmaskedtransformerwhichprocessesatokenizedversionof theinputyandproducesanoutput := (y),where
 RM d .Morespecifically, thetransformer isimplementedfromNtransformerblocksconsistingofglobalself-attention
 layers,layer-normalizationandposition-wiseMLPsasfollows2:
 2adaptedfromhttps://github.com/lucidrains/x-transformers
 24
LDM-1 LDM-2 LDM-4 LDM-8 LDM-16 LDM-32
 z-shape 256 256 3 128 128 2 64 64 3 32 32 4 16 16 8 88 8 32
 Z- 2048 8192 16384 16384 16384
 Diffusionsteps 1000 1000 1000 1000 1000 1000
 NoiseSchedule linear linear linear linear linear linear
 ModelSize 270M 265M 274M 258M 260M 258M
 Channels 192 192 224 256 256 256
 Depth 2 2 2 2 2 2
 ChannelMultiplier 1,1,2,2,4,4 1,2,2,4,4 1,2,3,4 1,2,4 1,2,4 1,2,4
 Attentionresolutions 32,16,8 32,16,8 32,16,8 32,16,8 16,8,4 8,4,2
 HeadChannels 32 32 32 32 32 32
 BatchSize 9 11 48 96 128 128
 Iterations 500k 500k 500k 500k 500k 500k
 LearningRate 9e-5 1.1e-4 9.6e-5 9.6e-5 1.3e-4 1.3e-4
 Table14. HyperparametersfortheunconditionalLDMstrainedontheCelebAdatasetfortheanalysisinFig.7.Allmodelstrainedona
 singleNVIDIAA100. :Allmodelsaretrainedfor500kiterations. Ifconvergingearlier,weusedthebestcheckpointforassessingthe
 providedFIDscores.
 Task Text-to-Image Layout-to-Image Class-Label-to-Image SuperResolution Inpainting Semantic-Map-to-Image
 Dataset LAION OpenImages COCO ImageNet ImageNet Places Landscapes
 f 8 4 8 4 4 4 8
 z-shape 32 32 4 64 64 3 32 32 4 64 64 3 64 64 3 64 64 3 32 32 4
 Z- 8192 16384 8192 8192 8192 16384
 Diffusionsteps 1000 1000 1000 1000 1000 1000 1000
 NoiseSchedule linear linear linear linear linear linear linear
 ModelSize 1.45B 306M 345M 395M 169M 215M 215M
 Channels 320 128 192 192 160 128 128
 Depth 2 2 2 2 2 2 2
 ChannelMultiplier 1,2,4,4 1,2,3,4 1,2,4 1,2,3,5 1,2,2,4 1,4,8 1,4,8
 NumberofHeads 8 1 1 1 1 1 1
 Dropout-- 0.1---
BatchSize 680 24 48 1200 64 128 48
 Iterations 390K 4.4M 170K 178K 860K 360K 360K
 LearningRate 1.0e-4 4.8e-5 4.8e-5 1.0e-4 6.4e-5 1.0e-6 4.8e-5
 Conditioning CA CA CA CA concat concat concat
 (C)A-resolutions 32,16,8 32,16,8 32,16,8 32,16,8--
EmbeddingDimension 1280 512 512 512--
TransformerDepth 1 3 2 1--
Table15. HyperparametersfortheconditionalLDMsfromSec.4.AllmodelstrainedonasingleNVIDIAA100exceptfortheinpainting
 modelwhichwastrainedoneightV100.
 TokEmb(y)+PosEmb(y) (18)
 fori=1 N:
 1 LayerNorm( ) (19)
 2 MultiHeadSelfAttention( 1)+ (20)
 3 LayerNorm( 2) (21)
 MLP( 3)+ 2 (22)
 LayerNorm( ) (23)
 (24)
 With available, theconditioningismappedintotheUNetviathecross-attentionmechanismasdepictedinFig.3.We
 modifythe“ablatedUNet”[15]architectureandreplace theself-attentionlayerwithashallow(unmasked) transformer
 consistingofTblockswithalternatinglayersof(i)self-attention,(ii)aposition-wiseMLPand(iii)across-attentionlayer;
 25
see Tab. 16. Note that without (ii) and (iii), this architecture is equivalent to the “ablated UNet”.
 While it would be possible to increase the representational power of by additionally conditioning on the time step t, we
 do not pursue this choice as it reduces the speed of inference. We leave a more detailed analysis of this modification to future
 work.
 For the text-to-image model, we rely on a publicly available3 tokenizer [99]. The layout-to-image model discretizes the
 spatial locations of the bounding boxes and encodes each box as a (l b c)-tuple, where l denotes the (discrete) top-left and b
 the bottom-right position. Class information is contained in c.
 See Tab. 17 for the hyperparameters of and Tab. 13 for those of the UNet for both of the above tasks.
 Note that the class-conditional model as described in Sec. 4.1 is also implemented via cross-attention, where is a single
 learnable embedding layer with a dimensionality of 512, mapping classes y to 
R1 512.
 input
 Rh w c
 LayerNorm
 Conv1x1
 Reshape
 SelfAttention
 MLP
 T
 CrossAttention
 Reshape
 Conv1x1
 Rh w c
 Rh w dnh
 Rhw dnh
 Rhw dnh
 Rhw dnh
 Rhw dnh
 Rh w dnh
 Rh w c
 Table 16. Architecture of a transformer block as described in Sec. E.2.1, replacing the self-attention layer of the standard “ablated UNet”
 architecture [15]. Here, nh denotes the number of attention heads and d the dimensionality per head.
 Text-to-Image Layout-to-Image
 seq-length
 depth N
 dim
 77
 32
 1280
 92
 16
 512
 Table 17. Hyperparameters for the experiments with transformer encoders in Sec. 4.3.
 E.2.2 Inpainting
 For our experiments on image-inpainting in Sec. 4.5, we used the code of [88] to generate synthetic masks. We use a fixed
 set of 2k validation and 30k testing samples from Places [108]. During training, we use random crops of size 256 256
 and evaluate on crops of size 512 512. This follows the training and testing protocol in [88] and reproduces their reported
 metrics (see in Tab. 7). We include additional qualitative results of LDM-4, w/ attn in Fig. 21 and of LDM-4, w/o attn, big,
 w/ ft in Fig. 22.
 E.3. Evaluation Details
 This section provides additional details on evaluation for the experiments shown in Sec. 4.
 E.3.1 Quantitative Results in Unconditional and Class-Conditional Image Synthesis
 Wefollow commonpractice andestimate the statistics for calculating the FID-, Precision- and Recall-scores [29,50] shown in
 Tab. 1 and 10 based on 50k samples from our models and the entire training set of each of the shown datasets. For calculating
 FID scores we use the torch-fidelity package [60]. However, since different data processing pipelines might lead to
 different results [64], we also evaluate our models with the script provided by Dhariwal and Nichol [15]. We find that results
 3https://huggingface.co/transformers/model_doc/bert.html#berttokenizerfast
 26
mainly coincide, except for the ImageNet and LSUN-Bedrooms datasets, where we notice slightly varying scores of 7.76
 (torch-fidelity) vs. 7.77 (Nichol and Dhariwal) and 2.95 vs 3.0. For the future we emphasize the importance of a
 unified procedure for sample quality assessment. Precision and Recall are also computed by using the script provided by
 Nichol and Dhariwal.
 E.3.2 Text-to-Image Synthesis
 Following the evaluation protocol of [66] we compute FID and Inception Score for the Text-to-Image models from Tab. 2 by
 comparing generated samples with 30000 samples from the validation set of the MS-COCO dataset [51]. FID and Inception
 Scores are computed with torch-fidelity.
 E.3.3 Layout-to-Image Synthesis
 For assessing the sample quality of our Layout-to-Image models from Tab. 9 on the COCO dataset, we follow common
 practice [37,87,89] and compute FID scores the 2048 unaugmented examples of the COCO Segmentation Challenge split.
 To obtain better comparability, we use the exact same samples as in [37]. For the OpenImages dataset we similarly follow
 their protocol and use 2048 center-cropped test images from the validation set.
 E.3.4 Super Resolution
 We evaluate the super-resolution models on ImageNet following the pipeline suggested in [72], i.e. images with a shorter
 size less than 256 px are removed (both for training and evaluation). On ImageNet, the low-resolution images are produced
 using bicubic interpolation with anti-aliasing. FIDs are evaluated using torch-fidelity [60], and we produce samples
 on the validation split. For FID scores, we additionally compare to reference features computed on the train split, see Tab. 5
 and Tab. 11.
 E.3.5 Efficiency Analysis
 For efficiency reasons we compute the sample quality metrics plotted in Fig. 6, 17 and 7 based on 5k samples. Therefore,
 the results might vary from those shown in Tab. 1 and 10. All models have a comparable number of parameters as provided
 in Tab. 13 and 14. We maximize the learning rates of the individual models such that they still train stably. Therefore, the
 learning rates slightly vary between different runs cf. Tab. 13 and 14.
 E.3.6 User Study
 For the results of the user study presented in Tab. 4 we followed the protocoll of [72] and and use the 2-alternative force-choice
 paradigm to assess human preference scores for two distinct tasks. In Task-1 subjects were shown a low resolution/masked
 image between the corresponding ground truth high resolution/unmasked version and a synthesized image, which was gen
erated by using the middle image as conditioning. For SuperResolution subjects were asked: ’Which of the two images is a
 better high quality version of the low resolution image in the middle?’. For Inpainting we asked ’Which of the two images
 contains more realistic inpainted regions of the image in the middle?’. In Task-2, humans were similarly shown the low
res/masked version and asked for preference between two corresponding images generated by the two competing methods.
 As in [72] humans viewed the images for 3 seconds before responding.
 27
F.ComputationalRequirements
 Method Generator Classifier Overall Inference Nparams FID IS Precision Recall
 Compute Compute Compute Throughput
 LSUNChurches2562
 StyleGAN2[42] 64- 64- 59M 3.86--
LDM-8(ours,100steps,410K) 18- 18 6.80 256M 4.02- 0.64 0.52
 LSUNBedrooms2562
 ADM[15] (1000steps) 232- 232 0.03 552M 1.9- 0.66 0.51
 LDM-4(ours,200steps,1.9M) 60- 55 1.07 274M 2.95- 0.66 0.48
 CelebA-HQ2562
 LDM-4(ours,500steps,410K) 14.4- 14.4 0.43 274M 5.11- 0.72 0.49
 FFHQ2562
 StyleGAN2[42] 32.13- 32.13- 59M 3.8--
LDM-4(ours,200steps,635K) 26- 26 1.07 274M 4.98- 0.73 0.50
 ImageNet2562
 VQGAN-f-4(ours,firststage) 29- 29- 55M 0.58--
VQGAN-f-8(ours,firststage) 66- 66- 68M 1.14--
BigGAN-deep[3] 128-256 128-256- 340M 6.95 203.6 2.6 0.87 0.28
 ADM[15](250steps) 916- 916 0.12 554M 10.94 100.98 0.69 0.63
 ADM-G[15](25steps) 916 46 962 0.7 608M 5.58- 0.81 0.49
 ADM-G[15](250steps) 916 46 962 0.07 608M 4.59 186.7 0.82 0.52
 ADM-G,ADM-U[15](250steps) 329 30 349 n/a n/a 3.85 221.72 0.84 0.53
 LDM-8-G(ours,100,2.9M) 79 12 91 1.93 506M 8.11 190.4 2.6 0.83 0.36
 LDM-8(ours,200ddimsteps2.9M,batchsize64) 79- 79 1.9 395M 17.41 72.92 0.65 0.62
 LDM-4(ours,250ddimsteps178K,batchsize1200) 271- 271 0.7 400M 10.56 103.49 1.24 0.71 0.62
 LDM-4-G(ours,250ddimsteps178K,batchsize1200,classifier-freeguidance[32]scale1.25) 271- 271 0.4 400M 3.95 178.22 2.43 0.81 0.55
 LDM-4-G(ours,250ddimsteps178K,batchsize1200,classifier-freeguidance[32]scale1.5) 271- 271 0.4 400M 3.60 247.67 5.59 0.87 0.48
 Table18. Comparingcomputerequirementsduringtrainingandinferencethroughputwithstate-of-the-artgenerativemodels.Compute
 duringtraininginV100-days,numbersofcompetingmethodstakenfrom[15]unlessstateddifferently; :Throughputmeasuredinsam
ples/seconasingleNVIDIAA100; :Numberstakenfrom[15]; :Assumedtobetrainedon25Mtrainexamples; :R-FIDvs. ImageNet
 validationset
 InTab18weprovideamoredetailedanalysisonourusedcomputeressourcesandcompareourbestperformingmodels
 ontheCelebA-HQ,FFHQ,LSUNandImageNetdatasetswiththerecent stateof theartmodelsbyusingtheirprovided
 numbers,cf.[15].Astheyreport theirusedcomputeinV100daysandwetrainallourmodelsonasingleNVIDIAA100
 GPU,weconverttheA100daystoV100daysbyassuminga 22speedupofA100vsV100[74]4.Toassesssamplequality,
 weadditionallyreportFIDscoresonthereporteddatasets.Wecloselyreachtheperformanceofstateoftheartmethodsas
 StyleGAN2[42]andADM[15]whilesignificantlyreducingtherequiredcomputeresources.
 4ThisfactorcorrespondstothespeedupoftheA100overtheV100foraU-Net,asdefinedinFig.1in [74]
 28
G. Details on Autoencoder Models
 We train all our autoencoder models in an adversarial manner following [23], such that a patch-based discriminator D
 is optimized to differentiate original images from reconstructions D(E(x)). To avoid arbitrarily scaled latent spaces, we
 regularize the latent z to be zero centered and obtain small variance by introducing an regularizing loss term Lreg.
 We investigate two different regularization methods: (i) a low-weighted Kullback-Leibler-term between qE(zx) =
 N(z;E E 2)andastandard normal distribution N(z;01) as in a standard variational autoencoder [46,69], and, (ii) regu
larizing the latent space with a vector quantization layer by learning a codebook of Zdifferent exemplars [96].
 To obtain high-fidelity reconstructions we only use a very small regularization for both scenarios, i.e. we either weight the
 KLterm by a factor 10 6 orchoose a high codebook dimensionality Z.
 The full objective to train the autoencoding model (ED) reads:
 LAutoencoder = min
 ED 
max Lrec(xD(E(x))) Ladv(D(E(x)))+logD(x)+Lreg(x;ED)
 (25)
 DMTraininginLatentSpace Notethatfortraining diffusion models on the learned latent space, we again distinguish two
 cases whenlearning p(z) or p(zy) (Sec. 4.3): (i) For a KL-regularized latent space, we sample z = E (x)+E (x) =: E(x),
 where 
N(01). When rescaling the latent, we estimate the component-wise variance
 2 = 1
 bchw bchw
 (zbchw 
from the first batch in the data, where = 1
 bchw
 )2
 bchw zbchw. The output of E is scaled such that the rescaled latent has
 unit standard deviation, i.e. z z= E(x)
 . (ii) For a VQ-regularized latent space, we extract z before the quantization layer
 and absorb the quantization operation into the decoder, i.e. it can be interpreted as the first layer of D.
 H. Additional Qualitative Results
 Finally, we provide additional qualitative results for our landscapes model (Fig. 12, 23, 24 and 25), our class-conditional
 ImageNet model (Fig. 26- 27) and our unconditional models for the CelebA-HQ, FFHQ and LSUN datasets (Fig. 28- 31).
 Similar as for the inpainting model in Sec. 4.5 we also fine-tuned the semantic landscapes model from Sec. 4.3.2 directly on
 5122 images and depict qualitative results in Fig. 12 and Fig. 23. For our those models trained on comparably small datasets,
 we additionally show nearest neighbors in VGG [79] feature space for samples from our models in Fig. 32- 34.
 29
bicubic
 LDM-BSR
 Figure 19. LDM-BSR generalizes to arbitrary inputs and can be used as a general-purpose upsampler, upscaling samples from the LSUN
Cows dataset to 10242 resolution.
 30
input
 GT
 Pixel Baseline #1
 Pixel Baseline #2
 LDM#1
 LDM#2
 Figure 20. Qualitative superresolution comparison of two random samples between LDM-SR and baseline-diffusionmodel in Pixelspace.
 Evaluated on imagenet validation-set after same amount of training steps.
 31
input
 GT
 LaMa [88]
 LDM#1
 LDM#2
 LDM#3
 Figure 21. Qualitative results on image inpainting. In contrast to [88], our generative approach enables generation of multiple diverse
 samples for a given input.
 32
input
 result
 input
 result
 Figure 22. More qualitative results on object removal as in Fig. 11.
 33
Semantic Synthesis on Flickr-Landscapes [23] (5122 finetuning)
 Figure 23. Convolutional samples from the semantic landscapes model as in Sec. 4.3.2, finetuned on 5122 images.
 34
Figure 24. A LDM trained on 2562 resolution can generalize to larger resolution for spatially conditioned tasks such as semantic synthesis
 of landscape images. See Sec. 4.3.2.
 35
Semantic Synthesis on Flickr-Landscapes [23]
 Figure 25. When provided a semantic map as conditioning, our LDMs generalize to substantially larger resolutions than those seen during
 training. Although this model was trained on inputs of size 2562 it can be used to create high-resolution samples as the ones shown here,
 which are of resolution 1024 384.
 36
Random class conditional samples on the ImageNet dataset
 Figure 26. Random samples from LDM-4 trained on the ImageNet dataset. Sampled with classifier-free guidance [32] scale s = 50 and
 200 DDIMsteps with = 10.
 37
Random class conditional samples on the ImageNet dataset
 Figure 27. Random samples from LDM-4 trained on the ImageNet dataset. Sampled with classifier-free guidance [32] scale s = 30 and
 200 DDIMsteps with = 10.
 38
Random samples on the CelebA-HQ dataset
 Figure 28. Random samples of our best performing model LDM-4 on the CelebA-HQ dataset. Sampled with 500 DDIM steps and = 0
 (FID = 5.15).
 39
Random samples on the FFHQ dataset
 Figure 29. Random samples of our best performing model LDM-4 on the FFHQ dataset. Sampled with 200 DDIM steps and = 1 (FID
 = 4.98).
 40
Random samples on the LSUN-Churches dataset
 Figure 30. Random samples of our best performing model LDM-8 on the LSUN-Churches dataset. Sampled with 200 DDIM steps and
 =0(FID=4.48).
 41
Random samples on the LSUN-Bedrooms dataset
 Figure 31. Random samples of our best performing model LDM-4 on the LSUN-Bedrooms dataset. Sampled with 200 DDIM steps and
 =1(FID=2.95).
 42
Nearest Neighbors on the CelebA-HQ dataset
 Figure 32. Nearest neighbors of our best CelebA-HQ model, computed in the feature space of a VGG-16 [79]. The leftmost sample is
 from our model. The remaining samples in each row are its 10 nearest neighbors.
 43
Nearest Neighbors on the FFHQ dataset
 Figure 33. Nearest neighbors of our best FFHQ model, computed in the feature space of a VGG-16 [79]. The leftmost sample is from our
 model. The remaining samples in each row are its 10 nearest neighbors.
 44
Nearest Neighbors on the LSUN-Churches dataset
 Figure 34. Nearest neighbors of our best LSUN-Churches model, computed in the feature space of a VGG-16 [79]. The leftmost sample
 is from our model. The remaining samples in each row are its 10 nearest neighbors.
 45